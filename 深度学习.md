# 深度学习

## 1 回归与分类

### 1.1 Logistic 回归

1. 定义

   目标：给定数据点 $X^{(n)}∈R^m$ 和相应标签 $t^{(n)}∈Ω$ ，找到一个映射 $f:R^m→Ω$

   - 回归的目的是预测一个<font color='blue'>连续的数值变量</font>，如果Ω是一个<font color='blue'>连续</font>的集合称其为<font color='red'>回归(regression)</font>
   - 分类的目的是将数据<font color='blue'>划分为离散的类</font>，如果Ω是一个<font color='blue'>离散</font>的集合称其为<font color='red'>分类(classification)</font>

2. 回归类型

   - 线性回归：用于建立因变量和自变量之间线性关系的统计方法
     $$
     f(x) =w x + \beta
     $$
     其中，$y$是因变量，$x_1, x_2, \cdots, x_n$是自变量，$\beta_0, \beta_1, \cdots, \beta_n$是回归系数。

   - 多项式回归：多项式回归是一种扩展了线性回归的方法，它可以拟合因变量和自变量之间的非线性关系。
     $$
     f(x) = \beta + w_1 x + w_2 x^2 + w_3 x^3 + \cdots + w_m x^m
     $$
     其中，$m$是多项式的最高次数。

   通过均方误差 （$MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$）进行训练，得到最终的 $f(x)$。

3. 分类方法

   - 线性回归分类：感知机、SVM

   - 非线性回归分类：<font color='red'>sigmoid function</font>
     $$
     f(x) = \frac{1}{1+e^{-x}}
     $$
     ![image-20240124165916191](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240124165916191.png)

   - 伯努利分布假设
     $$
     P(x)=\begin{cases}p,x=1\\
     	1-p,x=0
     \end{cases}
     $$
     

4. Logistic 回归

   <font color='blue'>Logistic 回归</font>是一种用于<font color='red'>二分类问题</font>的模型，它可以预测一个离散输出，例如0或1。

   - **Logistic回归函数**

     对于<font color='blue'>二分类问题</font>，一个0-1单元足以表示一个<font color='red'>标签</font>
     $$
     P(t=1|x)=\frac{1}{1+e^{-θ^\top x}}\triangleq h(x)
     $$
     其中$x$是输入，$t$ 是标签，$θ$ 是参数。我们的目标是寻找一个$θ$值使得概率$P(t=1|x)=h(x)$​。

     我们实质上是在<font color='blue'>用另一个连续函数来“回归”一个离散的函数</font>(x→t）

   - **最大化条件数据似然**

     最大化条件数据似然是一种参数估计方法，它利用已知的数据和条件分布，找到最有可能（即最大概率）导致这种分布的参数值。

     将t看作一个<font color='red'>伯努利变量</font>，并且$P(t=1|x)=h(x;\theta)$​。条件似然函数为
     $$
     P(t^{(1)},...,t^{(n)}|X;\theta)=\prod_{n=1}^{N}h(x^{(n)})^{t^{(n)}}(1-h(x^{(n)})^{1-t^{(n)}}
     $$
     <font color='red'>最大化似然等</font>同于最小化下式：
     $$
     E(θ)=-\frac{1}{n}lnP(t^{(1)},...,t^{(n)})=-\frac{1}{n}\sum^{n}_{n=1}\left(t^{(n)}ln\ h(x^{(n)}+(1-t^{(n)})ln\ (1-h(x^{(n)})\right)
     $$

   - **交叉熵误差函数**

     对于带有二元标签的一组训练样本$\{(x^{(n)},t^{(n)}):n=1,...,N\}$，定义<font color='red'>交叉熵误差（cross-entropyerror）函数</font>
     $$
     E(θ)=-\frac{1}{n}lnP(t^{(1)},...,t^{(n)})=-\frac{1}{n}\sum^{n}_{n=1}\left(t^{(n)}ln\ h(x^{(n)}+(1-t^{(n)})ln\ (1-h(x^{(n)})\right)
     $$

5. 训练和测试

   - 计算梯度
     $$
     \nabla E(\theta)=\frac{1}{N}\sum_Nx^{(n)}(h(x^{(n)})-t^{(n)})
     $$

   - 一些正则化项添加到成本函数中
     $$
     J(\theta)=E(\theta)+\lambda|\theta|^2/2
     $$

   - <font color='blue'>训练</font>：学习θ来最小化成本函数，其中$\alpha$是学习率。
     $$
     \theta \leftarrow \theta-\alpha \nabla J(\theta)
     $$

   - <font color='blue'>测试</font>：对于新的输入$x$，如果$P(t=1|x)>P(t=0|x)$，则可以预测输入为类别1，否则就是类别0。

### 1.2 Softmax 回归

1. 类别标签的表示

   **<font color='red'>one-hot编码</font>**(1-of-K)：将离散的类别标签转换为向量形式，其中每个类别都由一个唯一的二进制值表示。

   对于一个具有 $K$ 个可能类别的问题，1-of-K 表示法将每个类别映射为一个长度为 $K$ 的二进制向量，其中只有一个元素为1，其余为0。被设置为1的位置对应于类别的索引。

   > 例如，对于一个三类分类问题（$K = 3$），类别A、B和C可能被表示为：
   >
   > - 类别A：$1, 0, 0$
   > - 类别B：$0, 1, 0$
   > - 类别C：$0, 0, 1$

   - **唯一性：** 每个类别的表示是唯一的，因为只有一个元素为1。
   - **独立性：** 每个类别的表示与其他类别的表示是相互独立的，不存在冗余信息。

2. 分布假设
   - 正态分布假设

     **正态分布假**设是指假设数据集服从正态分布的概率分布。
     $$
     f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
     $$
     其中，$\mu$ 是均值（分布的中心），$\sigma$ 是标准差（度量分布的离散程度）。

   - Multinoulli分布假设

     **Multinoulli分布假设**描述了离散型随机变量的概率分布，特别适用于多类别分类问题。

     对于一个给定的样本，其类别的概率分布可以由参数 $\phi_k$ 来表示。对于一个离散型随机变量 $X$ 表示类别的取值，其概率质量函数如下：
     $$
     P(X=k) = \phi_k
     $$
     这表示样本属于类别 $k$ 的概率为 $\phi_k$ 。

3. Softmax 函数

   Softmax回归，也称为多类逻辑回归或多类交叉熵分类，是一种用于多类别分类的模型。

   假设有 $K$ 个类别，对于输入特征向量$x$​ ，Softmax回归的模型表达式如下：
   $$
   P(y=k \mid x) = \frac{e^{w_k \cdot x + b_k}}{\sum_{j=1}^{K} e^{w_j \cdot x + b_j}}
   $$
   其中，$P(y=k \mid x)$ 是给定输入 $x$ 属于类别 $k$ 的概率。$ w_k $ 和 $ b_k $ 是模型的参数，分别表示第 $k$个类别的权重和偏置。

4. 最大条件似然

   最大条件似然的目标是找到一组参数，使得在给定输入 $x$ 的条件下，观察到实际类别 $y$ 的概率最大。Softmax 回归的最大条件似然目标函数为：
   $$
   P(t^{(1)},...,t^{(N)}|X)=\prod_{n=1}^N\prod_{k=1}^KP(t_k^{(n)}=1|x^{(n)})^{t_k^{(n)}}
   $$
   其中：

   - $N$ 是样本数量。
   - $t_k^{(n)}$ 是one-hot函数，当 $t_k$ 等于 $k$ 时为1，否则为0。

5. 交叉摘误差函数

   Softmax回归通常使用交叉熵损失函数来衡量模型预测与实际类别之间的差异。对于 $N$​ 个样本，交叉熵损失函数的表达式为：
   $$
   J(w, b) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} t_k^{(n)} \ln P(t_k^{(n)}=1|x^{(n)})
   $$
   其中：

   - $J(w, b) $ 是损失函数。
   - $ t_k^{(n)} $ 是样本$n$ 属于类别 $k$ 的实际标签（1或0）。

   - $P(t_k^{(n)}=1|x^{(n)})$ 是模型对样本 $i$ 属于类别 $k$ 的预测概率。

6. 计算梯度

   Softmax 回归的梯度计算涉及对损失函数关于模型参数的偏导数。
   $$
   \frac{\part E^{(n)}}{\part \theta^{(k)}}=\delta_k^{(n)}x^{(n)}\\
   \delta_k^{(n)}=\frac{\part E^{(n)}}{\part u^{(k)}}=-(t_k^{(n)}-h_k^{(n)})
   $$
   上式是局部敏感度或局部梯度。

7. 训练和测试

   - 训练：训练Softmax回归的过程通常涉及使用<font color='blue'>梯度下降</font>或其变种来<font color='blue'>最小化交叉熵损失</font>。梯度下降的步骤包括计算损失函数对参数的梯度，然后更新参数以减小损失。

   $$
   \theta ← \theta -\alpha \nabla J(\theta)
	$$
- 测试：在个类别中找出 $P(k=1|x$) 最大的类别，作为新输入 $x$ 的预测标签

**总结**

![image-20240124180021695](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240124180021695.png)

### 1.3 **案例1：Softmax实现手写数字识别**

**相关知识点: numpy科学计算包，如向量化操作，广播机制等**

**1 简介**

本次案例中，你需要用python实现Softmax回归方法，用于MNIST手写数字数据集分类任务。你需要完成前向计算loss和参数更新。

你需要首先实现Softmax函数和交叉熵损失函数的计算。
$$
y=softmax(W^Tx+b)\\
L=CrossEntropy(y,label)
$$
在更新参数的过程中，你需要实现参数梯度的计算，并按照随机梯度下降法来更新参数。
$$
\frac{\partial L}{\partial W},\frac{\partial L}{\partial b}
$$
  具体计算方法可自行推导，或参照第三章课件。

 


**2 MNIST数据集**

 MNIST手写数字数据集是机器学习领域中广泛使用的图像分类数据集。它包含60,000个训练样本和10,000个测试样本。这些数字已进行尺寸规格化，并在固定尺寸的图像中居中。每个样本都是一个784×1的矩阵，是从原始的28×28灰度图像转换而来的。MNIST中的数字范围是0到9。下面显示了一些示例。 注意：在训练期间，切勿以任何形式使用有关测试样本的信息。

![image.png](https://raw.githubusercontent.com/ZzDarker/figure/main/img/13f138b5-4ccb-435d-a956-d3ffa3e6a6c0.png)

**3 任务要求**

1. **代码清单**

a) data/ 文件夹：存放MNIST数据集。你需要下载数据，解压后存放于该文件夹下。下载链接见文末，解压后的数据为 *ubyte 形式；

b) solver.py 这个文件中实现了训练和测试的流程。建议从这个文件开始阅读代码；

c) dataloader.py 实现了数据加载器，可用于准备数据以进行训练和测试；

d) visualize.py 实现了plot_loss_and_acc函数，该函数可用于绘制损失和准确率曲线；

e) optimizer.py 你需要实现带momentum的SGD优化器，可用于执行参数更新；

f) loss.py 你需要实现softmax_cross_entropy_loss，包含loss的计算和梯度计算；

g) runner.ipynb 完成所有代码后的执行文件，执行训练和测试过程。

 

**2.** **要求**

 我们提供了完整的代码框架，你只需要完成**optimizer.py，loss.py** 中的 **#TODO**部分。你需要提交整个代码文件和带有结果的**runner.ipynb (****不要提交数据集****)** 并且附一个**pdf**格式报告，内容包括：

a) 记录训练和测试的准确率。画出训练损失和准确率曲线；

b) 比较使用和不使用momentum结果的不同，可以从训练时间，收敛性和准确率等方面讨论差异；

c) 调整其他超参数，如学习率，Batchsize等，观察这些超参数如何影响分类性能。写下观察结果并将这些新结果记录在报告中。

 

**4** **其他**

1. 注意代码的执行效率，尽量不要使用for循环；
2. 不要在**pdf**报告中粘贴很多代码(只能包含关键代码)，对添加的代码作出解释;
3. 不要使用任何深度学习框架，如TensorFlow，Pytorch等；
4. 禁止抄袭。

 

**5** **参考**

1. 数据集下载：[http://yann.lecun.com/exdb/mnist/index.html](http://yann.lecun.com/exdb/mnist/)

## 2 多层感知机

### 2.1 前向计算

1. 多层感知机（Multi-layer Perceptron，MLP）

   ![image-20240129225229441](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240129225229441.png)

   - 除了输入共L层
   - 连接类型：
     - 相邻层之间的神经元两两连接
     - 相邻层之间没有反馈连接
     - 同一层内没有横向连接
   - 每个神经元接收前一层神经元的输出，并根据激活函数作出响应

2. 前向传播

   - $l=1,...,L-1$层

     $l$ 层的神经元 $j$ 的<font color='blue'>输入</font>：$u_j^{(l)}=\sum_jw_{ji}^{(l)}y_i^{(l-1)}+b_j^{(l)}$，注意$y^{(0)}=x$

     $l$ 层的神经元 $j$ 的<font color='blue'>输出</font>：$y_j^{(l)}=f(u_j^{(l)})$；$f(·)$是激活函数

   - $l=L$层：对应任务层（$Softmax$​分类、回归、图像去噪）

3. 激活函数

   <img src="https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240129230216709.png" alt="image-20240129230216709" style="zoom:50%;" />

   - Sigmoid函数
     $$
     f(z)=\frac{1}{1+e^{-z}}
     $$
     梯度：$f^\prime (z)=f(z)(1-f(z))$
   
   - 双曲正切函数
     $$
     f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
     $$
     
   
     梯度：$f^\prime (z)=1-f(z)^2$
   
      - 整流线性激活函数（ReLU）
        $$
        f(z)=max(0,z)
        $$
        梯度：$f^\prime (z)=\begin{cases}1,if\ z≥0\\0,else\end{cases}$

4. 向量形式的前向计算

   如果前一层有N个神经元，当前层有M个神经元，则定义参数矩阵和偏置向量如下：
   $$
   W^{(l)}=\begin{pmatrix}
       W_{11} & \cdots & w_{1N} \\
       \vdots & \vdots & \vdots \\
       W_{M1} & \cdots & w_{MN} \\
   \end{pmatrix},
   b^{(l)}=\begin{pmatrix}
       b_1 \\
       \vdots  \\
       b_M &  \\
   \end{pmatrix}
   $$
   对于$l=1,...,L-1$
   $$
   u^{(l)}=W{(l)}y{(l-1)}+b{(l)}and\ y{(l)}=f(u^{(l)})
   $$

### 2.3 反向传播

1. 损失函数
   $$
   E=\frac{1}{N}\sum_{n=1}^NE^{(n)}
   $$
   其中$E^{(n)}$是每个样本$n$的损失函数。

   - 均方误差
     $$
     E^{(n)}=\frac{1}{2}\sum_{k=1}^K(t_k-y_k^{L})^2,y_k^{(L)}=\frac{1}{1+e^{-z}},z=w_k^{(L)\top}y^{(L-1)}+b_k^{L}
     $$

   - 交叉熵误差
     $$
     E^{(n)}=-\sum_{k=1}^Kt_kln\ y_k^{(L)},y_k^{(L)}=\frac{e^{z_k}}{\sum_{j=1}^Ke^{z_j}},z_k=w_k^{(L)\top}y^{(L-1)}+b_k^{L}
     $$
     其中t是目标向量

2. 参数更新

   - 更新参数$w_{ji}^{(l)}$和$b_j$
     $$
     w_{ji}^{(l)}=w_{ji}^{(l)}-\alpha \frac{\partial E}{\partial w_{ji}^{(l)}}\\b_j^{(l)}=b_j^{(l)}-\alpha \frac{\partial E}{\partial b_j^{(l)}}
     $$

   - <font color='blue'>参数衰减</font>：通常作用于$w_{ji}^{(l)}$，相当于在损失函数加上额外一项：
     $$
     J=E+\frac{\lambda }{2}\sum_{i,j,l}(w_{ji}^{(l)})^2
     $$
     $w$的参数更新为：
     $$
     w_{ji}^{(l)}=w_{ji}^{(l)}-\alpha \frac{\partial E}{\partial w_{ji}^{(l)}}-\alpha \lambda w_{ji}^{(l)}
     $$

3. 梯度(Gradient)、局部敏感度(local sensitivity）

   - 局部敏感度
     $$
     \delta_i^{(l)}=\frac {\part E^{(n)}}{\part u_i^{(l)}}
     $$

   - 计算<font color='red'>梯度</font>等价于计算<font color='red'>各层的局部敏感度</font>
     $$
     \frac {\part E^{(n)}}{\part w_{ji}^{(l)}}=\delta_i^{(l)}\frac {\part u_j^{(l)}}{\part w_{ji}^{(l)}}=\delta_i^{(l)}f(u_i^{(l-1)})\\\frac {\part E^{(n)}}{\part b_j^{(l)}}=\delta_i^{(l)}
     $$

   - 多层感知机<font color='blue'>最后一层</font>的输出：
     $$
     y_k^{L}=f(u_k^{(L)})=f(w_k^{(L)\top}y^{(L-1)}+b_k^{(L)})
     $$
     $f$ 可以是 logistic sigmoid、tanh、ReLU

   - 其它层的局部敏感度
     $$
     \delta_i^{(l)}=\frac {\part E^{(n)}}{\part u_i^{(l)}}=\sum_j\delta_j^{(l+1)}w_{ji}^{(l+1)}f^\prime(u_i^{(l)})
     $$
     已知$l+1$层的$\delta$，可求$l$ 层的 $\delta$ 。因此可以在反向传播中计算$\delta_i^{(l)},\part E/\part W^{(l)},\part E/\part b^{(l)}$,以$l=L,L－1,…,1$的顺序

4. 向量形式的反向传播

   - 局部敏感度
     $$
     \delta_i^{(l)}=\left(\frac {\part E^{(n)}}{\part u_1^{(l)}},\frac {\part E^{(n)}}{\part u_2^{(l)}},...\right)
     $$

   - 对于<font color='blue'>输出层</font>$L$

     均方损失：$\delta^{(L)}=(y^{(L)}-t)\bigodot f^\prime (u^{(L)})$，其中$\bigodot$表示逐元素相乘。

     交叉熵损失：$\delta^{(L)}=y^{(L)}-t$

   - 对于<font color='blue'>隐含层</font>$1≤l＜L$
     $$
     \delta^{(l)}=(W^{(l+1)})^\top \delta^{(l+1)}\bigodot f^\prime (u^{(L)})
     $$

   - 计算<font color='blue'>梯度</font>$1≤l≤L$
     $$
     \frac {\part E^{(n)}}{\part W^{(l)}}=\delta^{(l)}f(u^{(l-1)}),\frac {\part E^{(n)}}{\part b^{(l)}}=\delta^{(l)}
     $$

   - 更新参数
     $$
     W^{(l)}=W^{(l)}-\frac{\alpha}{N}\sum_n \frac{\partial E}{\partial W^{(l)}}-\alpha \lambda W^{(l)},b^{(l)}=b^{(l)}-\frac{\alpha}{N}\sum_n \frac{\partial E}{\partial b^{(l)}}
     $$

5. <font color='blue'>梯度消失</font>(Gradient Vanishing)

   对于logistic、tanh这两个激活函数，从第L层到第一层，$\delta^{(l)}$变得越来越小。
   <font color='red'>浅层的梯度会逐渐接近于零</font>，<font color='blue'>RELU</font>可以缓解这个问题。

6. 多层感知机实现

   - 前向计算

     计算$f(u^l)$和$f^\prime (u^l)$，对所有的$l=1,2,…,L$

   - 后向计算

     计算$\delta ^{(l)}$和$\part E/\part W^{(l)},\part E/\part b^{(l)}$对所有的$l=L,L-1,...,1$

   - 更新参数

     对所有$1=1,2…,L,$更新$W^{(l)}$和$b^{(l)}$

   - 模块化编程

     - 用类来实现层，并提供前向和后向计算函数
     - 不同类型的层拥有不同的前向和后向计算函数，例如输入层、隐含层、softmax输出层、sigmoid输出层等
     - 通过组合不同的模块来构造不同的多层感知机模型

### 2.4 层分解

1. 灵活的层分解

   - 输入层或隐含层
     - 全连接层
     - 激活层
   - 平方损失函数层
     - 激活层
     - 损失层
   - 交叉损失层
     - Softmax层+损失层？不必要，因为激活层只能是Softmax层

2. 层分解举例

   - 带隐含层的多层感知机，使用平方误差损失

     ![image-20240130153908897](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130153908897.png)



### 2.5 训练技巧-1

1. 参数初始化

   W可以从某个分布中采样：

   - 高斯分布(Gaussian)
     均值为零，标准差固定(例如0.01)的高斯分布
   - Xavier
     均值为零，标准差固定为$1/\sqrt{n_{in}}$的一种分布,其中$n_{in}$是当前神经元输入的个数
     通常使用高斯分布或均匀分布
   - MSRA
     均值为零，标准差固定为$2/\sqrt{n_{in}}$的高斯分布

2. 学习率

   随机梯度下降中的学习率α通常远小于批量梯度下降中的，因为更新过程中存在更大的方差。

   - 选择一个<font color='blue'>足够小的学习率</font>使得网络开始<font color='blue'>稳定收敛</font>，在<font color='blue'>收敛速度变慢后将学习率减半</font>。
   - 每一轮训练后都在一组留出数据集(held out set)上<font color='blue'>评估模型</font>，如果相邻两轮训练中目标函数的改变量<font color='blue'>小于某个國值，则减小学习率</font>。
   - 在第t次迭代(iteration)中将学习率修改为$\frac{a}{b+t}$，其中a为初始学习率，b控制学习率何时开始衰减。

3. 训练样本的顺序

   - 如果样本的训练顺序固定，梯度的计算可能会有偏差，并导致收敛情况变差
   - 通常在每轮训练前都要随机打乱(random shuffle)数据集

4. 动量（Momentum）

   动量(Momentum)是一种迫使目标函数沿着“沟壑”快速下降的方法。

   ![image-20240130155030947](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130155030947.png)
   $$
   v=γv-α\nabla_\theta J(\theta;x^{(i)},t^{(i)})\\\theta=\theta+v
   $$

   - $v$是当前的速度向量
   - $γ∈(0,1]$决定了之前迭代的梯度以多大程度参与到当前的更新中。
   - 一种策略：$γ$设为0.5直到收敛开始稳定，然后增大到0.9或更大。

