# 机器学习

[TOC]



## 1 基本概念

### 1.1 概述

1. **什么是机器学习 **—— 在某种<font color='blue'>任务</font>上基于<font color='blue'>经验</font>不断<font color='blue'>进步</font>

   T (Task)：需要解决什么<font color='red'>任务</font>

   P(Performance)：任务确定什么<font color='red'>指标</font>

   E(Experience)：通过什么<font color='red'>经验</font>学习进步

2. 归纳学习假设

   任一假设若在<font color='blue'>足够大</font>的<font color='green'>训练样例集</font>中<font color='blue'>很好地逼近</font>目标函数， 它也能在<font color='red'>未见实例</font>中很好地逼近目标函数

3. 通用机器学习系统设计

   - 用于训练的经验——数据、训练过程、特征（训练数据偏差）

   - 到底应该学什么——**目标函数**：正确 vs 可行（<font color='blue'>假设</font>）

   - 应该如何表示——函数类型必须依据<font color='blue'>表达能力</font>仔细选取

   - 具体用什么算法去学习——最小均方误差、梯度下降法

   - 综合设计——<font color='orange'>数据→特征表示→算法→评价</font>

     ![image-20240122215205128](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122215205128.png)

4. 基本概念

   - **<font color='blue'>实例空间(Instance Space) X</font>**：例：每一天由一些属性描述 天空，空气温度，湿度，风，水，预报

   - **<font color='blue'>假设空间(Hypothesis Space) H</font>**：例：一个假设 if (温度 = 寒冷 AND 湿度 = 高) then 打网球 = 否

   - **<font color='blue'>训练样例空间(Sample Space) D</font>**：正例和负例 (<font color='red'>基于问题设定</font>）<x~1~,c(x~1~)> ,……, <x~m~,c(x~m~)>

   - **<font color='blue'>目标概念(Target Concept) C</font>**：假设$h∈H$，求 $ h(x)=c(x)for\;all\;x∈X $

     全部x的实例空间太大，换成$ h(x)=c(x)for\;all\;x∈D $

5. 有监督和无监督学习

   |          |                        有监督                         |                     无监督                     |
   | -------- | :---------------------------------------------------: | :--------------------------------------------: |
   | 训练样例 | <font color='blue'>(X,Y)对</font>，通常包含人为的努力 | <font color='blue'>仅 X </font>,通常不涉及人力 |
   | 学习目标 |     学习 X 和 Y 的<font color='blue'>关系</font>      |    学习 X 的<font color='blue'>结构</font>     |
   | 效果衡量 |                       损失函数                        |                       无                       |
   | 应用     |    <font color='blue'>预测</font>: X=输入, Y=输出     |     <font color='blue'>分析</font>: X=输入     |

### 1.2 机器学习实验方法与原则

#### 1.2.1 平均指标

1. 回归任务：预测值 $p_i$ 常为连续值，需要衡量与真实值 $y_i$ 之间的误差

   - <font color='red'>平均绝对误差（MAE）</font>
     $$
     MAE=\frac {1} {n}\sum_{i=1}^{n} {|y_i-p_i|}
     $$

   - <font color='red'>均方误差（MSE）</font>：预测误差较大的样本影响更大
     $$
     MSE=\frac {1} {n}\sum_{i=1}^{n} {(y_i-p_i)^2}
     $$

   - <font color='red'>均方根误差（RMSE）</font>：与预测值、标签单位相同
     $$
     RMSE=\sqrt{MSE}=\sqrt{\frac {1} {n}\sum_{i=1}^{n} {(y_i-p_i)^2}}
     $$
     

2. 分类任务：预测值一般为离散的类别，需要判断是否等于真实类别

   - 准确率（Accuracy）
     $$
     Accuracy=\frac {1} {n}\sum_{i=1}^{n} {(y_i=p_i)}
     $$

   - 错误率（Error Rate）
     $$
     Error\,Rate = 1-Accuracy=1-\frac {1} {n}\sum_{i=1}^{n} {(y_i=p_i)}
     $$

     > 以下为针对二分类任务的评价指标
     >
     > ![image-20240122230828785.png](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122230828785.png)

   - 精度（Precision）:预测为正例的样本中有多少确为正例
     $$
     Precision = \frac{TP}{TP+FP}
     $$

   - 召回率（Recall）：找到的真实正例占所有正例中的比例
     $$
     Recall = \frac{TP}{TP+FN}
     $$

   - 加权调和平均$F_\beta$​：
     $$
     F_\beta=1/[\frac{1}{1+\beta ^2}(\frac{1}{P}+\frac{\beta^2}{R})]\\
     F_1=\frac{2PR}{P+R}
     $$

   - <font color='red'>ROC曲线</font>：表示在不同阈值下模型的真阳性率（TPR）和假阳性率（FPR）之间的关系。

     ![image-20240122232003344](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122232003344.png)

     - 根据预测值对样本排序
     - 以该样本的预测值为阈值
     - 大于或等于阈值记正例，否则记负例可得到一组结果及评价指标，共有样本数n组结果
     - 假正例率（False Positive Rate，FPR）为横轴
     - 真正例率（True Positive Rate，TPR，也即召回率）为纵轴

   - <font color='red'>AUC（Area Under ROC Curve）</font>：ROC曲线下的面积，越大越好

     - 把测试样例以预测值从大到小排序，其中有n1个真实正例，n0个真实负例
     - 设 $r_i$ 为第 $i$ 个真实负例的秩（排序位置），$S_0=\sum r_i$

     $$
     AUC=\frac{S_0-n_0(n_0+1)/2}{n_0N_1}
     $$

     > ![image-20240122233627200](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122233627200.png)

3. 特定任务：

   - 个性化推荐：前K项精度（Precision@K）、前K项召回率（Recall@K）、前K项 命中率（Hit@K）等

   - 对话系统：BLEU、ROUGE、METEOR等
   - DCG(Discounted Cumulative Gain)：DCG 是对一个特定位次p的累积增益(Cumulative)

### 1.2.2 训练集、验证集与测试集

1. 训练集：模型可见样本标签，用于训练模型，样本数有限
2. 测试集：用于评估模型在可能出现的未见样本上的表现
3. 验证集：从<font color='blue'>训练集</font>中额外分出的集合，一般用于<font color='blue'>超参数</font>的调整（<font color='red'>防止过拟合</font>）

![image-20240122234843100](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122234843100.png)

### 1.2.3 随机重复实验

1. 数据随机性：由<font color='blue'>数据集划分</font>带来的评价指标波动
   - （数据足够多时）增多测试样本
   - （数据量有限时）重复多次划分数据集
2. 模型随机性：由<font color='blue'>模型或学习算法</font>本身带来的评价指标波动
   - 更改随机种子重复训练、测试
3. 报告结果：评价指标的均值$\bar X=\frac{1}{n}\sum_{i=1}^{n}X_i$
   - <font color='blue'>样本标准差</font>(个体离散程度，反映了个体对样本均值的代表性)$S=\sqrt{\sum_{i=1}^{n}(X_i-\bar X)^2/(n-1)}$
   - <font color='blue'>标准误差</font>(样本均值的离散程度，反映了样本均值对总体均值的代表性)$SEM=\frac{S}{\sqrt{n}}$

> 注意：保持每次得到的评价指标<font color='blue'>独立同分布(iid)</font>

### 1.2.4 K折交叉验证

​	随机把数据集分成K个<font color='blue'>相等大小的不相交</font>子集，K一般取5、10

![image-20240122235825060](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122235825060.png)

- 优点：<font color='blue'>数据利用率高</font>，适用于数据较少时
- 缺点：训练集互相有交集，<font color='blue'>每一轮之间并不满足独立同分布</font>
- 增大K，一般情况下：
  - 所估计的模型效果<font color='blue'>偏差（bias）下降</font>
  - 所估计的模型效果<font color='blue'>方差（variance）上升</font>
  - 计算代价上升，更多轮次、训练集更大

### 1.2.4 统计有效性检验

1. 抽样理论基础

   二项分布：描述了在n次次独立的伯努利试验中，成功的次数的离散情况。

   伯努利试验：成功概率: p，失败概率: q =1-p；n次试验中正好得到r次成功的概率为P(r)。
   $$
   P(r)=C_n^rp^r(1-p)^{n-r}=\frac{n!}{r!(n-r)!}p^r(1-p)^{n-r}
   $$
   ![image-20240125204758756](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240125204758756.png)

2. 效果估计

   给定一个假设在<font color='blue'>有限量数据</font>上的准确率，该准确率是否能准确估计在<font color='blue'>其它未见数据</font>上的效果？

   <font color='blue'>n 个随机样本中有 r 个被误分类的概率</font>——<font color='red'>二项分布</font>（样本的错误率=真实的错误率）
   $$
   真实错误率error_D(h)=p,样本错误率error_S(h)=r/n\\
   E[r]=np,E[error_S(h)]=E[r/n]=p=error_D(h)\\
   σ_{error_S(h)}=\frac{σ_r}{n}≈\sqrt{\frac{error_S(h)(1-error_S(h))}{n}}
   $$
   样本期望值=真实期望值；样本方差值 ≈ 真实方差值

   - 估计**<font color='blue'>偏差 （Bias）</font>**

     如果 S 是训练集, $error_S (h)$ 是有偏差的，bias指<font color='blue'>样本错误率的期望</font>与<font color='blue'>真实错误率</font>的差值
     $$
     \text{bias}=E[\text{error}_S(h)]-\text{error}_D(h)
     $$
     对于<font color='blue'>无偏估计(bias =0)</font>, h(训练集模型)和 S(测试集)必须<font color='blue'>独立不相关</font>地产生——<font color='red'>不要在训练集上测试！</font>

   - 估计**<font color='blue'>方差 （Varias）</font>**

     即使是S 的无偏估计, $error_S (h)$ 可能仍然和 $error_D (h)$ 不同，例：n=100,r=12;n=25,r=3错误率都为12%，但是方差分别为3.2%,6.5%

     需要选择<font color='red'>无偏</font>的且有<font color='red'>最小方差</font>的估计

3. 置信区间——准确率的估计可能包含多少错误？

   定义：参数p 的N %置信区间是一个以N %的概率包含p 的区间, N% : 置信度

   > 90.0%的置信度 ，年龄：[12, 24]
   >
   > 99.9%的置信度，年龄：[3, 60]

   - 如何得到置信区间?——通过<font color='blue'>正态分布</font>的某个<font color='red'>区间 （面积）</font>来获得

     ![image-20240125213425919](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240125213425919.png)

     均值$μ$有N%的可能性落在区间$y±Z_Nσ$

   - 中心极限定理——当<font color='blue'>样本量足够大</font>时，<font color='blue'>二项分布可以用正态分布来近似</font>。

     经验法则：$n>30, np(1-p)> 5$

     问题设定：

     a. <font color='blue'>独立同分布</font>的随机变量$Y_1,...,Y_n$；

     b. 未知分布，有均值$\mu$和有限方差$\sigma^2$；

     c. 估计均值为$\bar Y=\frac{1}{n}\sum_{i=1}^nY_i$，服从正态分布

     > 若S 包含 n >= 30个样本, 与h独立产生，且每个样本独立采样，则<font color='blue'>真实错误率$error_D$</font>落在以下区间有N% 置信度:
     > $$
     > error_S(h)±z_N\sqrt{\frac{error_S(h)(1-error_S(h))}{n}}
     > $$
     > 

4. 假设检验

   比较两个样本或一个样本和一个常数的均值差异是否显著

   - z检验

     Z检验通常用于大样本（样本容量大于30）或已知总体标准差的情况。Z值的计算方式为：

     $$
     Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
     $$

     - $\bar{X}$ 是样本均值。
     - $\mu$是总体均值。
     - $\sigma$是总体标准差。
     - $n$ 是样本容量。

     一般用于单次评测，随机变量为<font color='blue'>每个测试样本</font>的对错

   - t检验

     t检验适用于小样本（样本容量小于30）或总体标准差未知的情况。t值的计算方式为：
     $$
     t = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
     $$

     - $\bar{X}$ 是样本均值。
     - $\mu$是总体均值。
     - $s$是样本标准差。
     - $n$ 是样本容量。

     一般用于多次评测如重复实验，随机变量为<font color='blue'>每次测试集</font>上的指标

## 2 监督学习

![image-20240117212526879](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240117212526879.png)

### 2.1 决策树

#### 2.1.1 决策树学习基础

1. 适用决策树学习的经典目标问题

   - 带有<font color='blue'>非数值特征</font>的分类问题
   - <font color='blue'>离散</font>特征
   - <font color='blue'>没有相似度</font>概念
   - 特征<font color='blue'>无序</font>

   例子：

   > |  Sky  | Temp | Humid  |  Wind  | Water | Forecast | Enjoy |
   > | :---: | :--: | :----: | :----: | :---: | :------: | :---: |
   > | Sunny | Warm | Normal | Strong | Warm  |   Same   |  Yes  |
   > | Sunny | Warm |  High  | Strong | Warm  |   Same   |  Yes  |
   > | Rainy | Cold |  High  | Strong | Warm  |  Change  |  No   |
   > | Sunny | Warm |  High  | Strong | Cool  |  Change  |  Yes  |

2. 样本表示：<font color='red'>属性的列表</font>而非数值向量

3. 决策树概念

   ```mermaid
   flowchart TD
       A[Outlook] --> B(Sunny)
       A --> C(Overcast)
       A --> D(Rain)
       B --> E[Humidity]
       C --> F(Yes)
       D --> G[Wind]
       E --> High --> No
       E --> Normal -->Y[Yes]
       G --> Strong -->X[No]
       G --> Weak -->Yes
   ```

   - 分枝：特征/属性的取值
   - 非叶子节点：特征/属性
   - 叶子节点：决策/标签/类别/概念

#### 2.1.2 经典决策树算法

1. ID3算法

   ID3算法是一种用来构造决策树的<font color='blue'>贪心算法</font>，它是一种<font color='blue'>监督学习</font>的方法，可以用来进行分类预测。

   - 自顶向下，贪心搜索，递归算法
   - 核心循环
     - $A$ :下一步 <font color='blue'>最佳</font> 决策属性
     - 将 $A$ 作为当前节点决策属性
     - 对属性$A (v_i )$的每个值，创建与其对应的新的子节点
     - 根据属性值将训练样本分配到各个节点
     - 如果 <font color='blue'>训练样本被完美分类</font>，则退出循环，否则继续下探分裂新的叶节点

2. 当前最佳属性节点选择

   - 基本原则：<font color='red'>简洁</font>——我们偏向于使用简洁的具有较少节点的树

   - 属性选择和节点<font color='red'>**混杂度(Impurity)**</font>

     在每个节点 N上，我们选择一个属性 T，使得到达当 前派生节点的数据<font color='red'>尽可能 “纯”</font>

     - 熵(Entropy)
       $$
       Entropy(N)=-\sum_jP(w_j)log_2P(w_j)
       $$
       定义：$0log0=0$​

       在信息理论中，熵度量了信息的<font color='blue'>纯度/混杂度</font>，或者信息的 <font color='red'>不确定性</font>

       <font color='red'>正态分布 – 具有最大的熵值</font>

       ![image-20240130233111887](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233111887.png)

     - Gini 混杂度
       $$
       i(N)=1-\sum_iP^2(w_j)
       $$
       最大 Gini 混杂度 在 $1-n*(1/n)^2=1-1/n$​ 时取得

       ![image-20240130233134362](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233134362.png)

     - 错分类混杂度
       $$
       i(N)=1-max_jP(w_j)
       $$
       在有n类时，最大错分类混杂度 = 最大Gini混杂度 $1-max(1/n)=1-1/n$

   - 信息增益(IG)——度量<font color='blue'>混杂度的变化</font>ΔI(N)

     由于对A的排序整理带来的<font color='red'>熵的期望减少量</font>
     $$
     Gain(S,A)=Entropy(S)-\sum_{v∈Values(A)}\frac{|S_v|}{|S|}Entropy(S_v)
     $$
     ![image-20240130233041594](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233041594.png)

3. 停止分裂节点

   - 如果当前子集中所有数据 <font color='blue'>有完全相同的<font color='red'>输出类别</font></font>，那么终止

     ①有噪声、②漏掉重要Feature

   - 如果当前子集中所有数据 <font color='blue'>有完全相同的<font color='red'>输入特征</font></font>，那么终止

4. 假设空间

   - <font color='blue'>假设空间是完备的</font>——目标函数一定在假设空间里
   - <font color='blue'>输出单个假设</font>——不超过20个问题(根据经验)
   - <font color='blue'>没有回溯</font>——<font color='blue'>局部</font>最优
   - 在每一步中使用子集的<font color='blue'>所有数据</font>——数据驱动的搜索选择，对噪声数据有<font color='blue'>鲁棒性</font>

5. 归纳偏置（Inductive  Bias）

   - <font color='blue'>无假设空间限制</font>——假设空间 H 是作用在样本集合 X 上的
   - <font color='blue'>有搜索偏置</font>——偏向于在靠近根节点处的属性具有<font color='blue'>更大信息增益的树</font>（奥卡姆剃刀）

6. CART (分类和回归树)

   - 根据训练数据构建一棵决策树
   - 决策树会逐渐把训练集合分成越来越小的子集
   - 当子集纯净后不再分裂
   - 或者接受一个不完美的决策

7. ID3算法基本流程

   - 从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的划分特征；
   - 由该特征的不同取值建立子节点；
   - 再对子节点递归上述步骤，构建决策树；
   - 直到没有特征可以选择或类别完全相同为止，得到最终的决策树。

#### 2.1.3 过拟合问题

1. 定义

   ![image-20240130235345214](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130235345214.png)

   > a. 真实数据，b. 非过拟合结果，c. 过拟合结果

   当$h∈H$ <font color='blue'>对训练集过拟合</font>，如果 存在另一个假设$h’ ∈H$ 满足：$err_{train}(h)<err_{train}(h')\ AND\ err_{test}(h)＞err_{test}(h')$

   > 例子：每个叶节点都对应单个训练样本 —— 每个训练样本都被完美地分类（查表）

2. 解决方案——如何避免过拟合

   - <font color='blue'>预剪枝</font>——当数据的分裂在统计意义上并不显著时，就停止增长

     - <font color='red'>基于样本数</font>：通常 一个节点不再继续分裂，当到达一个节点的<font color='red'>训练样本数小于训练集合的一个特定比例 </font>(%5)，无论混杂度或错误率是多少都停止分裂

       原因：基于<font color='blue'>过少数据样本</font>的决定会带来<font color='blue'>较大误差和泛化错误</font>

     - 基于<font color='red'>信息增益的阈值</font>：设定一个较小的阈值，如果满足 $Δi(s)≤\beta$​ 则停止分裂

       优点：用到了所有训练数据；叶节点可能在树中的任何一层

       缺点：很难设定一个好的阈值

   - <font color='blue'>后剪枝</font>——构建一棵完全树，然后再剪枝

     - 错误降低剪枝：剪枝直到再剪就会对损害性能

       把数据集分为<font color='blue'>训练集</font>和<font color='blue'>验证集</font>，在<font color='blue'>验证集</font>上测试剪去每个可能节点(和以其为根的子树)的影响

       贪心地去掉某个可以<font color='blue'>提升验证集准确率</font>的节点

     - 规则后剪枝：

       - 把树转换成等价的由规则构成的集合
       - 对每条规则进行剪枝，去除哪些能够<font color='blue'>提升</font>该规则准确率的<font color='blue'>规则前件</font>
       - 将规则排序成一个序列 (根据规则的<font color='blue'>准确率从高往低排序</font>) 
       - 用<font color='blue'>该序列</font>中的最终规则对样本进行分类（<font color='blue'>依次查看</font>是否满足规则序列）

       注：在规则被剪枝后，它可能<font color='red'>不再能恢复成一棵树</font>

### 2.2 回归算法

#### 2.2.1 线性回归

1. 回归分析(Regression)

   回归分析是描述<font color='blue'>变量间关系</font>的一种统计分析方法

   > 例：在线教育场景
   >
   > - 因变量 Y：在线学习课程满意度 
   >
   > - 自变量 X：平台交互性、教学资源、课程设计

   预测性的建模技术，通常用于<font color='blue'>预测分析</font>，预测的结果多为<font color='blue'>连续值</font>（也可为离散值，二值）

2. 线性回归 (Linear regression)

   因变量和自变量之间是<font color='red'>线性关系</font>，就可以使用线性回归来建模
   $$
   y = b + m_1x_1 + m_2x_2 + \ldots + m_nx_n
   $$
   其中 $m_1, m_2, \ldots, m_n $ 是各个特征的权重，$ x_1, x_2, \ldots, x_n $是对应的特征值。

   线性回归的目的即找到<font color='blue'>最能匹配(解释)数据</font>的<font color='red'>截距</font>和<font color='red'>斜率</font>

3. 如何拟合数据

   假设只有一个因变量和自变量，每个训练样例表示 (𝑥𝑖 , 𝑦𝑖)

   用 $\hat y_i$ 表示根据拟合直线和 x𝑖 对 𝑦𝑖 的<font color='red'>预测</font>值：$\hat y_i=b_1+b_2x_i$

   定义 $e_i=y_i-\hat y_i$ 为<font color='red'>误差项</font>

   ![image-20240131105854324](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240131105854324.png)

   目标：得到一条直线使得对于<font color='blue'>所有训练样例的误差项尽可能小</font>

4. 线性回归的基本假设

   - 自变量与因变量间<font color='blue'>存在线性关系</font>；
   - 数据点之间<font color='blue'>独立</font>；
   - <font color='blue'>自变量之间无共线性</font>，相互独立；
   - <font color='blue'>残差独立</font>,等方差,且符合<font color='blue'>正态分布</font>。

#### 2.2.2 损失函数

1. 损失函数(loss function)

   损失函数（Loss Function）是用于衡量模型预测值与实际值之间差异的函数。

   - **均方误差（Mean Squared Error，MSE）**：用于回归问题，计算预测值与实际值之间的平方差的平均值。
     $$
     \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
     $$

   - **平均绝对误差（Mean Absolute Error，MAE）**：也用于回归问题，计算预测值与实际值之间的绝对差的平均值。
     $$
     \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
     $$

2. 最小二乘法（Least Square, LS)

   为了求解最优的截距和斜率，可以转化为一个针对损失函数的凸优化问题，称为<font color='red'>最小二乘法</font> 
   $$
   \min_{b_1,b_2}:\sum_{i=1}^n(y_i - \hat{y}_i)^2=\sum_{i=1}^n(y_i - b_1-b_2x)^2
   $$
   为求最小值，将上事对$b_1$，$b_2$求偏导，得到：
   $$
   b_2=\frac{\sum_{i=1}^n(x_i-\hat x)(y_i-\hat y)}{\sum_{i=1}^n(x_i-\hat x)^2}\\b_1=\hat y-b_2\hat x
   $$
   $\hat x$ 和 $\hat y$ 分别表示自变量和因变量的均值。

3. 梯度下降法(Gradient Descent, GD)

   沿着损失函数的梯度方向，通过不断更新模型参数来减小损失函数的值，直到达到损失函数的局部最小值或收敛到某个停止条件。

   - **初始化参数：** 随机初始化模型的参数（权重 $b_2$ 和偏置 $b_1$）。

   - **计算梯度：** 计算当前参数下损失函数关于每个参数的梯度（导数）。
     $$
     \frac{\part \sum_{i=1}^ne_i^2}{\part b}
     $$

   - **更新参数：** 沿着梯度的反方向调整参数，通过学习率 $\alpha$ 控制每次更新的步长。
     $$
     b=b-\alpha \frac{\part \sum_{i=1}^ne_i^2}{\part b}
     $$

   - **重复：** 重复步骤2和3，直到满足停止条件，例如达到最大迭代次数或梯度接近零。

   - **输出：** 返回最终的模型参数。

#### 2.2.3 多元线性回归

1. 多元线性回归(Multiple Linear Regression)

   当因变量有多个时，我们可以用矩阵方式表达
   $$
   Y=X\beta +e
   $$
   $Y$ 为输出，$X$ 为输入矩阵，$\beta$ 为回归系数，$e$ 是误差项（残差）。

2. 损失函数

   误差项 $e=y-X\beta$

   损失函数 $\sum_{i=1}^ne_i^2=e^\top e$，$e^\top$表示转置

   求解得到 $\beta =(X^\top X)^{-1}X^\top y$

3. 实例：家庭花销预测

   记录了25个家庭每年在快销品和日常服务上<font color='blue'>总开销($Y$)，每年固定收入($X_2$) ,持有流动资产($X_3$)</font>

   可以构建如下线性回归模型：
   $$
   y_i=\beta_1 +\beta_2x_{i2}+\beta_3x_{i3}+\epsilon_i
   $$
   带入 $\beta =(X^\top X)^{-1}X^\top y$ ，计算得到 $\beta_1=36.789$，$\beta_2=0.332$，$\beta_3=0.125$。

   预测：如果一个家庭每年固定收入为 50K\$、持有流动资产 100K\$，则 预计一年将会花费
   $$
   \hat y_i=36.79=0.332*50+0.125*100=65.96K
   $$

4. 以“误差平方和”为损失函数的优缺点

   - 优点
     - 损失函数是严格的凸函数，有<font color='red'>唯一解</font>
     - 求解过程简单且容易计算
   - 缺点
     - 结果对数据中的"<font color='red'>离群点"(outlier)</font>"非常<font color='red'>敏感</font>
     - 损失函数对于<font color='red'>超过</font>和<font color='red'>低于</font>真实值的预测是等价的

#### 2.2.4 线性回归的相关系数

1. 相关系数r

   定义因变量和自变量之间的<font color='blue'>相关系数 r</font>
   $$
   r=\frac{1}{n-1}\sum_{i=1}^n(\frac{x_i-\hat x}{s_x})(\frac{y_i-\hat y}{s_y})
   $$
   其中 $ \hat x$ 是 $X$ 的均值，$S_x$ 是 $X$ 的标准差$\sqrt{\frac{1}{n-1}\sum (x_i-\hat x)^2}$。

   协方差为$(\frac{x_i-\hat x}{s_x})(\frac{y_i-\hat y}{s_y})$，描述两个变量和Y的<font color='blue'>线性相关程度</font>。

   ![image-20240131114416003](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240131114416003.png)

   相关系数r的绝对值越接近1，线性相关程度越高。

2. 决定系数(coefficient of determination)

   决定系数$R^2$ ,也称作判定系数、拟合优度。

   <font color='blue'>决定系数</font>衡量了<font color='red'>模型对数据的解释程度</font>，y的波动有多少百分比能被x的波动所描述。
   $$
   R^2=1-\frac{\text{MSE}}{\text{VAR}}=\frac{\sum_i (y_i-\hat y)^2}{\sum_i(y_i-\bar y)^2}
   $$
   $y_i$ 为真实值，$\hat y_i$ 为预测值，$\bar y_i$ 为均值。$\text{MSE}$ 是均方误差，$\text{VAR}$是方差。

   $R^2$越接近1，表示回归分析中自变量对因变量的<font color='red'>解释越好</font>

   特别注意：<font color='red'>变量相关 ≠ 存在因果关系</font>。

### 2.3 贝叶斯学习

1. 贝叶斯学习背景

   试图发现<font color='blue'>两件事情的关系</font>（因果关系，先决条件&结论）。

   > 执果索因：肺炎→肺癌？不好确定，换成确诊肺癌得肺炎的概率

2. 贝叶斯定理

   贝叶斯定理是一种<font color='red'>用先验慨率来推断后验慨率</font>的公式，它可以表示为：
   $$
   P(h|D) = \frac{P(D|h)P(h)}{P(D)}
   $$

   - $P(h|D)$ 是**后验概率**，表示在已知事件 D 发生的情况下，事件 h 发生的概率；

   - $P(h)$​ 是 h 的**先验概率**，表示在没有任何其他信息的情况下，事件 h 发生的概率；

     $h$ 代表假设，应<font color='blue'>互相排斥</font>；且假设空间 $H$ 完全详尽，即 $\sum P(h_i)=1$

   - $P(D)$​​ 是**证据概率**，表示在没有任何其他信息的情况下，事件 D 发生的概率；

     $D$ 代表数据的一个采样集合，需要<font color='blue'>与 $h$ 独立</font>。

   - $P(D|h)$​ 是**似然概率**，表示在已知事件 h 发生的情况下，事件 D 发生的概率；

     在实践上往往取$log$ ，是可以得到的概率。

   > 举例：$h$ 代表得了癌症，$D$ 为测试结果为阳性。
   >
   > $P(h|D)$：已知测试结果为阳性，得癌症的概率。
   >
   > $P(D|h)$​：已知得了癌症，测试结果为阳性的概率。
   >
   > 我们已知：
   >
   > - 正确的阳性样本: 98% (患有该癌症, 测试结果为 +) 
   > - 正确的阴性样本: 97% (未患该癌症, 测试结果为 -)
   > - 在整个人群中，只有0.008 的人患这种癌症
   >
   > 如果一个人测试结果阳性，多大概率得癌症？
   > $$
   > \because P(+|\text{cancer})=0.98;P(\text{cancer})=0.008;P(-|\neg \text{cancer})=0.97;
   > \\\therefore P(+|\neg \text{cancer})=0.03;P(\neg \text{cancer})=0.992
   > \\P(+)=\sum_iP(+|h_i)P(h_i)=P(+|\text{cancer})P(\text{cancer})+P(+|\neg \text{cancer})P(\neg \text{cancer})
   > \\P(\text{cancer}|+)=\frac{P(+|\text{cancer})P(\text{cancer})}{P(+)}=\frac{0.98×0.008}{0.98×0.008+0.03×0.992}=0.21
   > $$

3. **最大后验假设MAP**(Max A Posterior)

   求在给定训练集上<font color='blue'>最有可能的假设</font>。
   $$
   h_{\text{MAP}}=\underset{h∈H}{\text{argmax}}\ P(D|h)P(h)
   $$
   $\underset{h∈H}{\text{argmax}}$ 指令后续公式取值最大的参数 $h$。

   最大后验概率的思想是，在有一些关于参数的先验知识的情况下，根据观测数据来修正参数的概率分布，并选择使后验概率最大的参数值作为估计值。

4. **极大似然假设ML**(Maximum Likelihood)

   如果我们<font color='red'>完全不知道假设的概率分布</font>，或者我们知道所有的<font color='red'>假设发生的概率相同</font>，那么MAP 等价于 <font color='blue'>极大似然假设 $h_{ML}$ (Maximum Likelihood)</font>，其公式为
   $$
   h_{ML}=\mathop{\arg\max}_{h_i∈H}\ P(D|H_i)
   $$

   - 最小二乘LSE

     最小二乘法（Least Squares Method），又称最小平方法，是一种数学优化方法，它通过最小化误差的平方和来找到数据的最佳函数匹配。假设训练数据为$<x_i,d_i>$
     $$
     d_i=f(x_i)+e_i
     $$
     $d_i$：独立的样本；$f(x)$：没有噪声的目标函数值；$e_i$​：噪声，独立随机变量，符合正态分布。

   - **极大似然和最小二乘法的关系**：
     $$
     \begin{align}
         h_{ML} & = \underset{h∈H}{\text{argmax}}\ P(D|h)P(h) \\
                   & = \underset{h∈H}{\text{argmax}}\ \prod_{i=1}^mp(d_i|h) \\ 
                   & = \underset{h∈H}{\text{argmax}}\ \prod_{i=1}^m\frac{1}{\sqrt{2π\sigma^2}}e^{-\frac{1}{2}(\frac{d_i-h(x_i)}{\sigma})^2}&\text{(正态分布)} \\ 
                   & = \underset{h∈H}{\text{argmax}}\ \sum_{i=1}^m \ln\frac{1}{\sqrt{2π\sigma^2}}-\frac{1}{2}(\frac{d_i-h(x_i)}{\sigma})^2&\text{(取ln,单调性)} \\ 
                   & = \underset{h∈H}{\text{argmax}}\ \sum_{i=1}^m (d_i-h(x_i))^2&\text{(最小二乘)}\\ 
                 
     \end{align}
     $$
     <font color='red'>在独立随机变量，正态分布噪声的情况下，$h_{ML}=h_{LSE}$</font>

5. 朴素贝叶斯NB

   朴素贝叶斯的核心思想是，根据贝叶斯定理，后验概率 P(Y|X) 与先验概率 P(Y) 和似然概率 P(X|Y) 成正比

   **朴素贝叶斯假设：**
   $$
   P(x|y_i)=P(a_1,a_2...a_n|v_j) =\prod_iP(a_i|v_j)
   $$
   $a_1,a_2...a_n$是<font color='blue'>相互独立</font>的属性，$v_j$某条件。

   **朴素贝叶斯分类器：**
   $$
   v_{\text{NB}}=\mathop{\arg\max}_{vi∈V}\{\log P(v_j)+\sum_i\log P(a_i|v_j) \}
   $$
   <font color='red'>如果满足属性之间的独立性，那么$v_{\text{MAP}}=v_{\text{NB}}$​</font>

   > - 举例1：词义消歧 (Word Sense Disambiguation)
   >
   >   对于单词 w，使用上下文 c 进行词义消歧
   >
   >   e.g. "A fly flies into the kitchen while he fry the chicken. "
   >
   >   如何判断fly的含义？<font color='blue'>根据上下文 $c$ 在词 $w$ 周围一组词 $w_i$ (特征)，进行判断词义$s_i$​</font>
   >
   >   朴素贝叶斯假设：$P(c|s_k) = \prod_{w_i∈c} P(w_i|s_k)$
   >
   >   朴素贝叶斯选择：$s=\underset{s_k}{argmax}\{\log P(s_k)+\sum_{w_i∈c}\log P(w_i|s_k) \}$
   >
   >   其中$P(s_k)=\frac{C(s_k)}{C(w)},P(w_i|s_k)=\frac{C(w_i,s_k)}{C(s_k)}$
   >
   > - 举例 2: 垃圾邮件过滤
   >
   >   经验：数据量要大；注重邮件头；不对词进行词干化；只用最显著的词；对假阳性做偏置

6. 最小描述长度MDL

   偏向假设 h 使得最小化
   $$
   h_{\text{MDL}}=\mathop{\arg\min}_{h∈H}\{L_{C_1}(h)+L_{C_2}(D|h) \}
   $$
   其中 $L_{C_x}$ 是 $x$ 在编码 $C$ 下的描述长度。

   <font color='blue'>为可能性较大的消息赋予较短的编码</font>

   在对信息编码时，更偏好 <font color='blue'>一个短的且错误更少的假设</font>，而不是一个长的但完美分类训练数据的假设

### 2.4 基于实例的学习

​	动机：人们通过<font color='red'>记忆</font>和行动来推理学习。

#### 2.4.1 基本概念与最近邻方法

1. 名词概念

   - 参数化

     设定一个特定的函数形式

     优点：<font color='blue'>简单</font>，容易估计和解释

     <font color='blue'>可能存在很大的偏置</font>：实际的数据分布可能不遵循假设的分布

   - 非参数化：

     分布或密度的估计是<font color='blue'>数据驱动</font>的（data-driven）

     需要事先对函数形式作的估计相对更少

2. 基于实例的学习

   无需构建模型一仅<font color='blue'>存储所有训练样例</font>，直到<font color='blue'>有新样例需要分类</font>才开始进行处理。

   - 一个概念 $c_i$ 可以表示为：

     <font color='blue'>样例</font>的集合 $c_i={e_{i1},e_{i2}...}$

     一个<font color='blue'>相似度估计函数</font> $f$

     一个<font color='blue'>阈值</font> $\theta$

   - 一个实例 $a$ 属于概念 $c_i$，当

     $a$ 和 $c_i$ 中的某些 $e_j$ 相似，并且 $f(e_i,a)>\theta$

3. 最近邻方法

   计算新的样例和每个样例的距离，找出最近距离的确定其分类。

   距离度量：欧式距离 $\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$

   ![image-20240205172643565](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205172643565.png)

   ![image-20240205172653858](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205172653858.png)

   1-NN的错误率不大于Bayes方法错误率的2倍

4. 最近邻的点是噪音怎么办?

   用不止一个邻居，在邻居中进行投票→K近邻（KNN）

#### 2.4.2 K-近邻（KNN）

1. 距离度量

   - **闵可夫斯基距离（Minkowski Distance）**

     这是一种广义的距离度量，它包含了欧氏距离、曼哈顿距离和切比雪夫距离等特例
     $$
     d (x,y) = \left (\sum_{i=1}^n |x_i - y_i|^p \right )^{\frac {1} {p}}
     $$
     其中，$x$ 和 $y$ 是两个 $n$ 维向量，$x_i$ 和 $y_i$ 是它们的第 $i$ 个分量，$p$ 是一个正数，表示距离的幂次。当 $p=2$ 时，就是欧氏距离；当 $p=1$ 时，就是曼哈顿距离；当 $p=\infty$ 时，就是切比雪夫距离。

   - **欧氏距离（Euclidean Distance）**

     这是最常用的距离度量，它表示两个点在空间中的直线距离，计算公式为：
     $$
     d (x,y) = \sqrt {\sum_{i=1}^n (x_i - y_i)^2}
     $$
     其中，$x$ 和 $y$ 是两个 $n$ 维向量，$x_i$ 和 $y_i$ 是它们的第 $i$ 个分量。

   - **曼哈顿距离（Manhattan Distance）**

     又称街区距离，这是另一种常用的距离度量，它表示两个点在网格中的路径距离，计算公式为：
     $$
     d (x,y) = \sum_{i=1}^n |x_i - y_i|
     $$
     其中，$x$ 和 $y$ 是两个 $n$ 维向量，$x_i$ 和 $y_i$ 是它们的第 $i$ 个分量。

   - **切比雪夫距离（Chebyshev Distance）**

     又称棋盘距离，这是一种极端的距离度量，它表示两个点在各个维度上的最大差值，计算公式为：
     $$
     d (x,y) = \max_{i=1}^n |x_i - y_i|
     $$
     其中，$x$ 和 $y$ 是两个 $n$ 维向量，$x_i$ 和 $y_i$ 是它们的第 $i$ 个分量。

   - **加权欧氏距离(Mean Censored Euclidean)** 
     $$
     d (x,y) = \sqrt {\frac{\sum_{i=1}^n (x_i - y_i)^2}{n}}
     $$
     在欧式距离中，如果维度很高那么最后的值会很大，加权欧式距离/n考虑的就是这个问题。

   - **Bray-Curtis Dist**
     $$
     \frac{\sum_k|x_{ik}-x_{jk}|}{\sum_k(x_{ik}+x_{jk})}
     $$
     在生物信息学上经常被使用，用来描述生物多样性。

2. 属性

   - 属性归一化

     目的是消除不同属性之间的量纲和尺度的影响，使得数据更加统一和规范

     归一化方法：$\log,\min-\max,sum$

   - 属性加权

     无关的属性也会被使用进来，需要根据每个属性的<font color='blue'>相关性</font>进行加权。

     加权方法：互信息
     $$
     I(X,Y)=H(X)+H(Y)-H(X,Y)&\text{H:熵(entropy)} \\
     H(X,Y)=-\sum p(x,y)\log p(x,y)&\text{联合熵(Joint entropy)}
     $$

3. 连续取值目标函数

   k个近邻训练样例的均值。

   ![image-20240205174546985](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205174546985.png)

   上图中<font color='red'>红色：实例的真实值</font>；<font color='blue'>蓝色：估计值</font>

4. k的选择

   - 多数情况下k=3

   - 取决于<font color='blue'>训练样例的数目</font>——更大的k不一定带来更好的效果

   - 交叉验证

     Leave-one-out：每次拿一个样例作为测试，所有其他的作为训练样例

   - KNN是<font color='blue'>稳定</font>的——样例中小的混乱不会对结果有非常大的影响

5. 打破平局

   如果k=3并且每个近邻都属于不同的类。

   - K值可以稍微比<font color='blue'>类别数大1或2</font>，并且为<font color='blue'>奇数</font>
   - 取1-NN（最近邻）
   - 随机选一个

6. 关于效率

   KNN算法把所有的计算放在新实例来到时，实时计算开销大

   - 加速对最近邻居的选择

     先检验<font color='blue'>临近的点</font>，忽略比目前找到最近的点更远的点

   - 通过 <font color='blue'>KD-tree</font> 来实现

     KD-tree: k 维度的树（数据点的维度是 k）

     基于树的数据结构

     递归地将点划分到和坐标轴平行的方形区域内

7. KD-Tree

   - 构建 KD-Tree

     ![image-20240205220055878](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205220055878.png)

     - 选择一个<font color='blue'>范围最宽的维度</font>，选择一个切分点，根据该维度的数据的<font color='red'>中位数</font>，将数据集分成两个子集，使得切分点左边的数据都小于等于它，右边的数据都大于等于它；
     - 递归地对左子树和右子树重复上述步骤，直到<font color='blue'>剩余的数据点少于 m，或者区域的宽度达到最小值</font>
     - 返回根节点，完成 KD-Tree 的构建。

     在每个叶节点维护一个额外信息：<font color='blue'>这个节点下所有数据点的 (紧) 边界</font>

   - 查询

     ![image-20240205220924635](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205220924635.png)

     - 先检验<font color='blue'>临近的点</font>：关注距离所查询数据点最近的树的分支

     - 达到一个叶节点后：计算节点中每个数据点距离目标点的距离

     - 接着<font color='blue'>回溯</font>检验我们访问过的每个树节点的另一个分支，每次找到一个最近的点，就更新<font color='blue'>距离的上界</font>
     - 利用<font color='blue'>最近距离</font>以及每个树节点下数据的<font color='blue'>边界信息</font>，对一部分<font color='blue'>不可能包含最近邻居</font>的分支<font color='blue'>进行剪枝</font>

8. KNN 优缺点

   - 优点：

     - 概念上很简单，但可以处理复杂的问题

     - 通过对k-近邻的平均，对<font color='blue'>噪声数据更鲁棒</font>

     - 容易理解：预测结果可解释

     - 训练样例中呈现的信息不会丢失，样例本身被<font color='blue'>显式</font>地存储下来

     - 实现简单、稳定、没有参数（除了 k）

   - 缺点

     - **内存开销大**：需要存储所有样例
     - **CPU 开销大**：分类新样本需要更大时间
     - 很难确定一个合适的**距离函数**
     - **不相关的特征** 对距离的度量有负面的影响

#### 2.4.3 距离加权 KNN

1. 加权函数
   $$
   w_i=K(d(x_i,x_q))
   $$
   其中$d(x_i,x_1)$是查询数据点与 $x_i$ 之间的关系；$K(·)$ 是决定每个数据点权重的核函数。

2. 输出
   $$
   \text{predict}=\frac{\sum w_iy_i}{\sum w_i}
   $$

3. 对比加权前后效果

   - KNN

     ![image-20240205222157055](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205222157055.png)

   - 距离加权KNN

     ![image-20240205222215211](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205222215211.png)

     高斯核函数：曲线更加平滑。

     ![image-20240205222234457](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205222234457.png)

#### 2.4.4 基于实例/记忆的学习器

1. 1-NN
   - 距离度量：欧式距离
   - 使用多少个邻居：一个
   -  加权函数（加权）：无
   - 如何使用已知的邻居节点：和邻居节点相同
2. K-NN
   - 距离度量：欧式距离
   - 使用多少个邻居：K 个
   - 加权函数（加权） ：无
   - 如何使用已知的邻居节点：K 个邻居节点投票
3. 距离加权 KNN
   - 距离度量：缩放的欧式距离
   - 使用多少个邻居：所有的，或K 个
   - 加权函数（加权） ：高斯核函数
   - 如何使用已知的邻居节点：每个输出的加权平均 

#### 2.4.5 局部加权回归

![image-20240205222719853](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205222719853.png)

- 距离度量：缩放的欧式距离

- 使用多少个邻居：所有的，或K 个

- 加权函数（加权） ：高斯核函数

- 如何使用已知的邻居节点：<font color='blue'>首先构建一个局部的线性模型。拟合 β 最小化局部的加权平方误差和 </font>
  $$
  \beta = \mathop{\arg\min}_\beta \sum_{k=1}^Nw_k^2(y_k-\beta^\top X_k)^2
  $$


#### 2.4.5 多种回归方式对比

1. **线性回归**

   ![image-20240205223054757](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223054757.png)

2. **连接所有点**

   ![image-20240205223401423](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223401423.png)

3. **1-近邻**

   ![image-20240205223411655](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223411655.png)

4. **k-近邻（k=9）**

   ![image-20240205223200166](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223200166.png)

5. **距离加权 KNN（核回归）**

   ![image-20240205223229347](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223229347.png)

   选择一个合适的 $K_w$ 非常重要，不仅是对核回归，对所有局部加权学习器都很重要。

6. **局部加权回归**

   ![image-20240205223307395](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240205223307395.png)

#### 2.4.6 懒惰学习与贪婪学习

1. 贪婪学习：查询之前就泛化
   - 训练时间长，测试时间短
   - 对于每个查询使用相同模型，倾向于给出全局估计
2. 懒惰学习：等待查询再泛化
   - 训练时间短，测试时间长
   - 可以得到局部估计

​	如果它们共享相同的假设空间，懒惰学习可以表示更复杂的函数

### 2.5 支持向量机（SVM）

#### 2.5.1 背景信息

1. 分类算法回顾

   - 决策树 

     - 样本的属性<font color='blue'>非数值</font>

     - 目标函数是<font color='blue'>离散</font>的 

   - 贝叶斯学习

     - 样本的属性可以是<font color='blue'>数值或非数值</font>
     - 目标函数是<font color='blue'>连续的（概率）</font>

   - K-近邻

     - 样本是<font color='blue'>空间（例如欧氏空间）中的点</font>
     - 目标函数可以是<font color='blue'>连续的也可以是离散的</font>

   - 支持向量机 (Support Vector Machine)

     - 样本是<font color='blue'>空间（例如欧氏空间）中的点</font>
     - 目标函数可以是<font color='blue'>连续的也可以是离散的</font>

2. 背景信息

   当前版本的支持向量机大部分是由 Vapnik 和他的同事在 AT&T贝尔实验室 开发的

   支持向量机 （Support Vector Machine，SVM）是一个<font color='red'>最大间隔分类器（Max Margin Classifier）</font>

   <font color='red'>最有效的监督学习方法之一</font>，曾被作为文本处理方法的一个强基准模型（strong baseline）

#### 2.5.2 线性支持向量机

1. 符号函数
   $$
   y_i =
           \begin{cases}
               +1,  & \text{if $f(x_i,θ)$ <0} \\
               -1, & \text{if $f(x_i,θ)$ <0} \\
           \end{cases}
   $$
   对一个测试样本 $x$，我们可以预测它的标签为$[f(x,θ)]$，$f(x,θ)=0$​ 被称为<font color='red'>分类超平面</font>

2. 线性分类器

   - 线性超平面

     $$
   f(x,w,b)=<x,w>+b=0
     $$
     
     在线性可分的情况下，有无穷多个满足条件的超平面。

     ![image-20240206180617328](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240206180617328.png)

   - 线性分类器的间隔(Margin)

     在分类分界面两侧分别放置<font color='blue'>平行</font>于分类超平面的一个超平面，<font color='blue'>移动超平面使其远离</font>分类超平面
     
     当他们各自<font color='blue'>第一次碰到</font>数据点时，他们之间的距离被称为线性分类器的<font color='blue'>间隔</font>
     
     <font color='red'>Margin（间隔）：分界在碰到数据点之前可以达到的宽度</font>
     
   - 最大间隔线性分类器——具有<font color='red'>最大间隔</font>的线形分类器

     支持向量：那些阻挡间隔继续扩大的数据点

     ![image-20240207011651499](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240207011651499.png)

   - 问题形式化

     形式化间隔，我们需要所有数据点满足
     $$
     y_i(<x_i,w>+b)≥1,\ \forall i=1,...,N
     $$
     ![image-20240207162957688](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240207162957688.png)

     分类超平面：$<x,w>+b=0$，引入平行于分类超平面的两个额外超平面：$<x,w>+b=±1$

     <font color='red'>间隔（margin）</font>：两个新的超平面($<x,w>+b=±1$)之间的距离。

     间隔的表达式：两个超平面到原点的距离之差的绝对值：$|ρ_1-ρ_2|=\frac{2}{|w|}$
     
     <font color='red'>最优化问题：使间隔最大化，即等价于最小化$w$</font>
     $$
     \mathop{max}_{w,b}\frac{2}{||w||_2}\Longleftrightarrow	
     \mathop{min}_{w,b}\frac{1}{2}||w||^2_2
     $$
     虽然看起来似乎间隔只与w有关，但<font color='blue'>b仍然通过约束w的取值，间接对间隔产生影响</font>

3. 形式化对偶问题

   - 原始问题
     $$
     \mathop{min}_{w,b}\frac{1}{2}||w||^2_2\\
     s.t.\ y_i(<w,x_i>+b)≥1,\forall i=1,...,N
     $$

   - 拉格朗日乘子法
     $$
     L(w,b,\alpha)=\frac{1}{2}||w||_2^2-\sum_i\alpha_i(y_i(<w,x_i>+b)-1)
     $$

   - KKT条件

     非线性规划领域里最重要的理论成果之一，是确定某点为极值点的必要条件。如果所讨论的规划是凸规划，那么KKT 条件也是充分条件。
     $$
     f(n)= \begin{cases} \frac{\partial L}{\partial w}=w-\sum_i\alpha_iy_ix_i=0 , & \alpha_i(y_i(<w,x_i>+b)-1)=0 \\ 
     \frac{\partial L}{\partial b}=\sum_iy_i\alpha_i=0 , & \alpha_i≥0 \end{cases}
     $$

   - 对偶问题

     把KKT条件带回拉格朗日乘子法，得到
     $$
     \mathop{max}_{\{a_i\}}\ -\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j<x_i,x_j>+\sum_i\alpha_i\\
     \mathop{min}_{\{a_i\}}\ \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j<x_i,x_j>-\sum_i\alpha_i\\
     s.t.\ \sum_iy_i\alpha_i=1,\alpha_i≥0,\forall i=1,...,N
     $$

   - 支持向量

     根据KKT条件，$\alpha_i$ 非零 当且仅当 $y_i(<w,x_i>+b)=1$，即 $x_i$ 在间隔边界上，这些 $x_i$是<font color='red'>支持向量(SV)</font>。

     ![image-20240220094508892](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240220094508892.png)

4. 线性不可分情况

   以上算法是线性可分情况下，线性不可分时，应允许分类错误。

   需要最小化$||w||^2_2$和分类错误
   $$
   \mathop{min}_{w,b}||w||^2_2+C×(\text{loss for errors})
   $$
   其中 $C>0$ 是一个用于平衡这二者的常数。

   - 损失函数

     ![image-20240220101237697](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240220101237697.png)

     - 0/1损失
       $$
       l_{0/1}(z_i)\triangleq1(z_i<0)
       $$
       非凸非连续，不容易求解。

     - Hinge损失
       $$
       l_{\text{hinge}}(z_i)\triangleq\mathop{max}(0,1-z_i)
       $$
       线性惩罚$z_i＜1$

     - 形式化损失函数

       引入松弛变量$\epsilon_i≥0$
       $$
       \mathop{min}_{w,b}\ ||w||^2_2+C\sum_{i=1}^N\epsilon_i\\
       s.t.\ \ y_i(<w,x_i>+b)≥1-\epsilon_i,\forall i=1,...,N,\epsilon_i≥0
       $$

   - 软间隔(soft margin)

     仍然希望找到最大间隔超平面，但此时：

     - 我们<font color='blue'>允许一些训练样本被错分类</font>
     - 我们<font color='blue'>允许一些训练样本在间隔区域内</font>

     ![image-20240220102153527](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240220102153527.png)

     > $\epsilon_i=0$时，数据点在间隔区域的<font color='red'>边界</font>上或在间隔区域<font color='red'>外部正确分</font>类的那一侧
     >
     > $0<\epsilon_i≤1$时，数据点在<font color='red'>间隔区域内</font>但在<font color='red'>正确分类</font>的一侧
     >
     > $\epsilon_i>1$时，数据点在分类超平面<font color='red'>错误分类</font>的一侧

     正值常数$C$平衡着<font color='blue'>大间隔和小分类错误</font>

   - 对偶问题
     $$
     \mathop{min}_{\{a_i\}}\ \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j<x_i,x_j>-\sum_i\alpha_i\\
     s.t.\ \sum_iy_i\alpha_i=1,C≥\alpha_i≥0,\forall i=1,...,N
     $$
     除了间隔边界上的数据点以外，那些在间隔区域内的、以及在错误一侧的数据点，也都是SV

#### 2.5.3 核函数支持向量机

1. 特征空间

   从输入空间(input space)到特征空间(feature space)映射。
   $$
   \phi(x):T^n→F
   $$
   样本$x_i$在<font color='blue'>输入空间中线性不可分</font>，但可能在<font color='red'>特征空间中线性可分</font>

   ![image-20240220104123833](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240220104123833.png)

   特征空间中的问题：

   ![image-20240220104319131](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240220104319131.png)

   <font color='red'>映射和特征空间都不唯一</font>

2. 核函数及其构造

   - 核函数

     为了在特征空间中求解对偶问题且找到分类超平面，我们只需要知道$<\phi(x),\phi(y)>$(内积)，而不是分别的$\phi(x)$，$\phi(y)$

     <font color='red'>核函数：$<\phi(x),\phi(y)>$</font>

   - 常用核函数

     - 齐次多项式Homogeneous polynomials：$k(x,y)=(<x,y>)^d$
     - 非齐次多项式Inhomogeneous polynomials：$k(x,y)=(<x,y>+1)^d$
     - 高斯核函数Gaussian Kernel：$k(x,y)=\mathop{exp}(-\frac{||x-y||^2}{2\sigma^2})$
     - Sigmoid核函数Sigmoid Kernel：$k(x,y)=\mathop{tanh}(\eta<x,y>+v)$

     > 更多常用核函数的列表: https://blog.csdn.net/chlele0105/article/details/17068949

3. 核函数构造

   - 选择一个特征函数$\phi(·)$，然后构造核函数$k(x,x')=<\phi(x),\phi(x')>$
   - 直接选择一个合理的核函数而不用显式构造$\phi(·)$
   - 利用简单核函数构造新的核函数

#### 2.5.4 SVM优缺点

1. 优点
   - 很好的数学基础
   - 最大化间隔使得方法的鲁棒性非常高，泛化能力强
   - 用线性的方法解决线性不可分问题
   - 实际应用不错
2. 缺点
   - 有多种核函数可用，针对具体问题，哪个核函数最好？一尚未找到理论上可证

## 3 无监督学习



### 3.1 聚类



## 4 集成学习



## 5 深度学习