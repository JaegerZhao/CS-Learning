# 机器学习

## 1 基本概念

### 1.1 概述

1. **什么是机器学习 **—— 在某种<font color='blue'>任务</font>上基于<font color='blue'>经验</font>不断<font color='blue'>进步</font>

   T (Task)：需要解决什么<font color='red'>任务</font>

   P(Performance)：任务确定什么<font color='red'>指标</font>

   E(Experience)：通过什么<font color='red'>经验</font>学习进步

2. 归纳学习假设

   任一假设若在<font color='blue'>足够大</font>的<font color='green'>训练样例集</font>中<font color='blue'>很好地逼近</font>目标函数， 它也能在<font color='red'>未见实例</font>中很好地逼近目标函数

3. 通用机器学习系统设计

   - 用于训练的经验——数据、训练过程、特征（训练数据偏差）

   - 到底应该学什么——**目标函数**：正确 vs 可行（<font color='blue'>假设</font>）

   - 应该如何表示——函数类型必须依据<font color='blue'>表达能力</font>仔细选取

   - 具体用什么算法去学习——最小均方误差、梯度下降法

   - 综合设计——<font color='orange'>数据→特征表示→算法→评价</font>

     ![image-20240122215205128](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122215205128.png)

4. 基本概念

   - **<font color='blue'>实例空间(Instance Space) X</font>**：例：每一天由一些属性描述 天空，空气温度，湿度，风，水，预报

   - **<font color='blue'>假设空间(Hypothesis Space) H</font>**：例：一个假设 if (温度 = 寒冷 AND 湿度 = 高) then 打网球 = 否

   - **<font color='blue'>训练样例空间(Sample Space) D</font>**：正例和负例 (<font color='red'>基于问题设定</font>）<x~1~,c(x~1~)> ,……, <x~m~,c(x~m~)>

   - **<font color='blue'>目标概念(Target Concept) C</font>**：假设$h∈H$，求 $ h(x)=c(x)for\;all\;x∈X $

     全部x的实例空间太大，换成$ h(x)=c(x)for\;all\;x∈D $

5. 有监督和无监督学习

   |          |                        有监督                         |                     无监督                     |
   | -------- | :---------------------------------------------------: | :--------------------------------------------: |
   | 训练样例 | <font color='blue'>(X,Y)对</font>，通常包含人为的努力 | <font color='blue'>仅 X </font>,通常不涉及人力 |
   | 学习目标 |     学习 X 和 Y 的<font color='blue'>关系</font>      |    学习 X 的<font color='blue'>结构</font>     |
   | 效果衡量 |                       损失函数                        |                       无                       |
   | 应用     |    <font color='blue'>预测</font>: X=输入, Y=输出     |     <font color='blue'>分析</font>: X=输入     |

### 1.2 机器学习实验方法与原则

#### 1.2.1 平均指标

1. 回归任务：预测值 $p_i$ 常为连续值，需要衡量与真实值 $y_i$ 之间的误差

   - <font color='red'>平均绝对误差（MAE）</font>
     $$
     MAE=\frac {1} {n}\sum_{i=1}^{n} {|y_i-p_i|}
     $$

   - <font color='red'>均方误差（MSE）</font>：预测误差较大的样本影响更大
     $$
     MSE=\frac {1} {n}\sum_{i=1}^{n} {(y_i-p_i)^2}
     $$

   - <font color='red'>均方根误差（RMSE）</font>：与预测值、标签单位相同
     $$
     RMSE=\sqrt{MSE}=\sqrt{\frac {1} {n}\sum_{i=1}^{n} {(y_i-p_i)^2}}
     $$
     

2. 分类任务：预测值一般为离散的类别，需要判断是否等于真实类别

   - 准确率（Accuracy）
     $$
     Accuracy=\frac {1} {n}\sum_{i=1}^{n} {(y_i=p_i)}
     $$

   - 错误率（Error Rate）
     $$
     Error\,Rate = 1-Accuracy=1-\frac {1} {n}\sum_{i=1}^{n} {(y_i=p_i)}
     $$

     > 以下为针对二分类任务的评价指标
     >
     > ![image-20240122230828785.png](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122230828785.png)

   - 精度（Precision）:预测为正例的样本中有多少确为正例
     $$
     Precision = \frac{TP}{TP+FP}
     $$

   - 召回率（Recall）：找到的真实正例占所有正例中的比例
     $$
     Recall = \frac{TP}{TP+FN}
     $$

   - 加权调和平均$F_\beta$​：
     $$
     F_\beta=1/[\frac{1}{1+\beta ^2}(\frac{1}{P}+\frac{\beta^2}{R})]\\
     F_1=\frac{2PR}{P+R}
     $$

   - <font color='red'>ROC曲线</font>：表示在不同阈值下模型的真阳性率（TPR）和假阳性率（FPR）之间的关系。

     ![image-20240122232003344](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122232003344.png)

     - 根据预测值对样本排序
     - 以该样本的预测值为阈值
     - 大于或等于阈值记正例，否则记负例可得到一组结果及评价指标，共有样本数n组结果
     - 假正例率（False Positive Rate，FPR）为横轴
     - 真正例率（True Positive Rate，TPR，也即召回率）为纵轴

   - <font color='red'>AUC（Area Under ROC Curve）</font>：ROC曲线下的面积，越大越好

     - 把测试样例以预测值从大到小排序，其中有n1个真实正例，n0个真实负例
     - 设 $r_i$ 为第 $i$ 个真实负例的秩（排序位置），$S_0=\sum r_i$

     $$
     AUC=\frac{S_0-n_0(n_0+1)/2}{n_0N_1}
     $$

     > ![image-20240122233627200](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122233627200.png)

3. 特定任务：

   - 个性化推荐：前K项精度（Precision@K）、前K项召回率（Recall@K）、前K项 命中率（Hit@K）等

   - 对话系统：BLEU、ROUGE、METEOR等
   - DCG(Discounted Cumulative Gain)：DCG 是对一个特定位次p的累积增益(Cumulative)

### 1.2.2 训练集、验证集与测试集

1. 训练集：模型可见样本标签，用于训练模型，样本数有限
2. 测试集：用于评估模型在可能出现的未见样本上的表现
3. 验证集：从<font color='blue'>训练集</font>中额外分出的集合，一般用于<font color='blue'>超参数</font>的调整（<font color='red'>防止过拟合</font>）

![image-20240122234843100](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122234843100.png)

### 1.2.3 随机重复实验

1. 数据随机性：由<font color='blue'>数据集划分</font>带来的评价指标波动
   - （数据足够多时）增多测试样本
   - （数据量有限时）重复多次划分数据集
2. 模型随机性：由<font color='blue'>模型或学习算法</font>本身带来的评价指标波动
   - 更改随机种子重复训练、测试
3. 报告结果：评价指标的均值$\bar X=\frac{1}{n}\sum_{i=1}^{n}X_i$
   - <font color='blue'>样本标准差</font>(个体离散程度，反映了个体对样本均值的代表性)$S=\sqrt{\sum_{i=1}^{n}(X_i-\bar X)^2/(n-1)}$
   - <font color='blue'>标准误差</font>(样本均值的离散程度，反映了样本均值对总体均值的代表性)$SEM=\frac{S}{\sqrt{n}}$

> 注意：保持每次得到的评价指标<font color='blue'>独立同分布(iid)</font>

### 1.2.4 K折交叉验证

​	随机把数据集分成K个<font color='blue'>相等大小的不相交</font>子集，K一般取5、10

![image-20240122235825060](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240122235825060.png)

- 优点：<font color='blue'>数据利用率高</font>，适用于数据较少时
- 缺点：训练集互相有交集，<font color='blue'>每一轮之间并不满足独立同分布</font>
- 增大K，一般情况下：
  - 所估计的模型效果<font color='blue'>偏差（bias）下降</font>
  - 所估计的模型效果<font color='blue'>方差（variance）上升</font>
  - 计算代价上升，更多轮次、训练集更大

### 1.2.4 统计有效性检验

1. 抽样理论基础

   二项分布：描述了在n次次独立的伯努利试验中，成功的次数的离散情况。

   伯努利试验：成功概率: p，失败概率: q =1-p；n次试验中正好得到r次成功的概率为P(r)。
   $$
   P(r)=C_n^rp^r(1-p)^{n-r}=\frac{n!}{r!(n-r)!}p^r(1-p)^{n-r}
   $$
   ![image-20240125204758756](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240125204758756.png)

2. 效果估计

   给定一个假设在<font color='blue'>有限量数据</font>上的准确率，该准确率是否能准确估计在<font color='blue'>其它未见数据</font>上的效果？

   <font color='blue'>n 个随机样本中有 r 个被误分类的概率</font>——<font color='red'>二项分布</font>（样本的错误率=真实的错误率）
   $$
   真实错误率error_D(h)=p,样本错误率error_S(h)=r/n\\
   E[r]=np,E[error_S(h)]=E[r/n]=p=error_D(h)\\
   σ_{error_S(h)}=\frac{σ_r}{n}≈\sqrt{\frac{error_S(h)(1-error_S(h))}{n}}
   $$
   样本期望值=真实期望值；样本方差值≈真实方差值

   - 估计**<font color='blue'>偏差 （Bias）</font>**

     如果 S 是训练集, $error_S (h)$ 是有偏差的，bias指<font color='blue'>样本错误率的期望</font>与<font color='blue'>真实错误率</font>的差值
     $$
     bias=E[error_s(h)]-error_D(h)
     $$
     对于<font color='blue'>无偏估计(bias =0)</font>, h(训练集模型)和 S(测试集)必须<font color='blue'>独立不相关</font>地产生——<font color='red'>不要在训练集上测试！</font>

   - 估计**<font color='blue'>方差 （Varias）</font>**

     即使是S 的无偏估计, $error_S (h)$ 可能仍然和 $error_D (h)$ 不同，例：n=100,r=12;n=25,r=3错误率都为12%，但是方差分别为3.2%,6.5%

     需要选择<font color='red'>无偏</font>的且有<font color='red'>最小方差</font>的估计

3. 置信区间——准确率的估计可能包含多少错误？

   定义：参数p 的N %置信区间是一个以N %的概率包含p 的区间, N% : 置信度

   > 90.0%的置信度 ，年龄：[12, 24]
   >
   > 99.9%的置信度，年龄：[3, 60]

   - 如何得到置信区间?——通过<font color='blue'>正态分布</font>的某个<font color='red'>区间 （面积）</font>来获得

     ![image-20240125213425919](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240125213425919.png)

     均值$μ$有N%的可能性落在区间$y±Z_Nσ$

   - 中心极限定理——当<font color='blue'>样本量足够大</font>时，<font color='blue'>二项分布可以用正态分布来近似</font>。

     经验法则：$n>30, np(1-p)> 5$

     问题设定：

     a. <font color='blue'>独立同分布</font>的随机变量$Y_1,...,Y_n$；

     b. 未知分布，有均值$\mu$和有限方差$\sigma^2$；

     c. 估计均值为$\bar Y=\frac{1}{n}\sum_{i=1}^nY_i$，服从正态分布

     > 若S 包含 n >= 30个样本, 与h独立产生，且每个样本独立采样，则<font color='blue'>真实错误率$error_D$</font>落在以下区间有N% 置信度:
     > $$
     > error_S(h)±z_N\sqrt{\frac{error_S(h)(1-error_S(h))}{n}}
     > $$
     > 

4. 假设检验

   比较两个样本或一个样本和一个常数的均值差异是否显著

   - z检验

     Z检验通常用于大样本（样本容量大于30）或已知总体标准差的情况。Z值的计算方式为：

     $$
     Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
     $$

     - $\bar{X}$ 是样本均值。
     - $\mu$是总体均值。
     - $\sigma$是总体标准差。
     - $n$ 是样本容量。

     一般用于单次评测，随机变量为<font color='blue'>每个测试样本</font>的对错

   - t检验

     t检验适用于小样本（样本容量小于30）或总体标准差未知的情况。t值的计算方式为：
     $$
     t = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
     $$

     - $\bar{X}$ 是样本均值。
     - $\mu$是总体均值。
     - $s$是样本标准差。
     - $n$ 是样本容量。

     一般用于多次评测如重复实验，随机变量为<font color='blue'>每次测试集</font>上的指标

## 2 监督学习

![image-20240117212526879](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240117212526879.png)

### 2.1 决策树

#### 2.1.1 决策树学习基础

1. 适用决策树学习的经典目标问题

   - 带有<font color='blue'>非数值特征</font>的分类问题
   - <font color='blue'>离散</font>特征
   - <font color='blue'>没有相似度</font>概念
   - 特征<font color='blue'>无序</font>

   例子：

   > |  Sky  | Temp | Humid  |  Wind  | Water | Forecast | Enjoy |
   > | :---: | :--: | :----: | :----: | :---: | :------: | :---: |
   > | Sunny | Warm | Normal | Strong | Warm  |   Same   |  Yes  |
   > | Sunny | Warm |  High  | Strong | Warm  |   Same   |  Yes  |
   > | Rainy | Cold |  High  | Strong | Warm  |  Change  |  No   |
   > | Sunny | Warm |  High  | Strong | Cool  |  Change  |  Yes  |

2. 样本表示：<font color='red'>属性的列表</font>而非数值向量

3. 决策树概念

   ```mermaid
   flowchart TD
       A[Outlook] --> B(Sunny)
       A --> C(Overcast)
       A --> D(Rain)
       B --> E[Humidity]
       C --> F(Yes)
       D --> G[Wind]
       E --> High --> No
       E --> Normal -->Y[Yes]
       G --> Strong -->X[No]
       G --> Weak -->Yes
   ```

   - 分枝：特征/属性的取值
   - 非叶子节点：特征/属性
   - 叶子节点：决策/标签/类别/概念

#### 2.1.2 经典决策树算法

1. ID3算法

   ID3算法是一种用来构造决策树的<font color='blue'>贪心算法</font>，它是一种<font color='blue'>监督学习</font>的方法，可以用来进行分类预测。

   - 自顶向下，贪心搜索，递归算法
   - 核心循环
     - $A$ :下一步 <font color='blue'>最佳</font> 决策属性
     - 将 $A$ 作为当前节点决策属性
     - 对属性$A (v_i )$的每个值，创建与其对应的新的子节点
     - 根据属性值将训练样本分配到各个节点
     - 如果 <font color='blue'>训练样本被完美分类</font>，则退出循环，否则继续下探分裂新的叶节点

2. 当前最佳属性节点选择

   - 基本原则：<font color='red'>简洁</font>——我们偏向于使用简洁的具有较少节点的树

   - 属性选择和节点<font color='red'>**混杂度(Impurity)**</font>

     在每个节点 N上，我们选择一个属性 T，使得到达当 前派生节点的数据<font color='red'>尽可能 “纯”</font>

     - 熵(Entropy)
       $$
       Entropy(N)=-\sum_jP(w_j)log_2P(w_j)
       $$
       定义：$0log0=0$​

       在信息理论中，熵度量了信息的<font color='blue'>纯度/混杂度</font>，或者信息的 <font color='red'>不确定性</font>

       <font color='red'>正态分布 – 具有最大的熵值</font>

       ![image-20240130233111887](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233111887.png)

     - Gini 混杂度
       $$
       i(N)=1-\sum_iP^2(w_j)
       $$
       最大 Gini 混杂度 在 $1-n*(1/n)^2=1-1/n$​ 时取得

       ![image-20240130233134362](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233134362.png)

     - 错分类混杂度
       $$
       i(N)=1-max_jP(w_j)
       $$
       在有n类时，最大错分类混杂度 = 最大Gini混杂度 $1-max(1/n)=1-1/n$

   - 信息增益(IG)——度量<font color='blue'>混杂度的变化</font>ΔI(N)

     由于对A的排序整理带来的<font color='red'>熵的期望减少量</font>
     $$
     Gain(S,A)=Entropy(S)-\sum_{v∈Values(A)}\frac{|S_v|}{|S|}Entropy(S_v)
     $$
     ![image-20240130233041594](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130233041594.png)

3. 停止分裂节点

   - 如果当前子集中所有数据 <font color='blue'>有完全相同的<font color='red'>输出类别</font></font>，那么终止

     ①有噪声、②漏掉重要Feature

   - 如果当前子集中所有数据 <font color='blue'>有完全相同的<font color='red'>输入特征</font></font>，那么终止

4. 假设空间

   - <font color='blue'>假设空间是完备的</font>——目标函数一定在假设空间里
   - <font color='blue'>输出单个假设</font>——不超过20个问题(根据经验)
   - <font color='blue'>没有回溯</font>——<font color='blue'>局部</font>最优
   - 在每一步中使用子集的<font color='blue'>所有数据</font>——数据驱动的搜索选择，对噪声数据有<font color='blue'>鲁棒性</font>

5. 归纳偏置（Inductive  Bias）

   - <font color='blue'>无假设空间限制</font>——假设空间 H 是作用在样本集合 X 上的
   - <font color='blue'>有搜索偏置</font>——偏向于在靠近根节点处的属性具有<font color='blue'>更大信息增益的树</font>（奥卡姆剃刀）

6. CART (分类和回归树)

   - 根据训练数据构建一棵决策树
   - 决策树会逐渐把训练集合分成越来越小的子集
   - 当子集纯净后不再分裂
   - 或者接受一个不完美的决策

7. ID3算法基本流程

   - 从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的划分特征；
   - 由该特征的不同取值建立子节点；
   - 再对子节点递归上述步骤，构建决策树；
   - 直到没有特征可以选择或类别完全相同为止，得到最终的决策树。

#### 2.1.3 过拟合问题

1. 定义

   ![image-20240130235345214](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240130235345214.png)

   > a. 真实数据，b. 非过拟合结果，c. 过拟合结果

   当$h∈H$ <font color='blue'>对训练集过拟合</font>，如果 存在另一个假设$h’ ∈H$ 满足：$err_{train}(h)<err_{train}(h')\ AND\ err_{test}(h)＞err_{test}(h')$

   > 例子：每个叶节点都对应单个训练样本 —— 每个训练样本都被完美地分类（查表）

2. 解决方案——如何避免过拟合

   - <font color='blue'>预剪枝</font>——当数据的分裂在统计意义上并不显著时，就停止增长

     - <font color='red'>基于样本数</font>：通常 一个节点不再继续分裂，当到达一个节点的<font color='red'>训练样本数小于训练集合的一个特定比例 </font>(%5)，无论混杂度或错误率是多少都停止分裂

       原因：基于<font color='blue'>过少数据样本</font>的决定会带来<font color='blue'>较大误差和泛化错误</font>

     - 基于<font color='red'>信息增益的阈值</font>：设定一个较小的阈值，如果满足 $Δi(s)≤\beta$​ 则停止分裂

       优点：用到了所有训练数据；叶节点可能在树中的任何一层

       缺点：很难设定一个好的阈值

   - <font color='blue'>后剪枝</font>——构建一棵完全树，然后再剪枝

     - 错误降低剪枝：剪枝直到再剪就会对损害性能

       把数据集分为<font color='blue'>训练集</font>和<font color='blue'>验证集</font>，在<font color='blue'>验证集</font>上测试剪去每个可能节点(和以其为根的子树)的影响

       贪心地去掉某个可以<font color='blue'>提升验证集准确率</font>的节点

     - 规则后剪枝：

       - 把树转换成等价的由规则构成的集合
       - 对每条规则进行剪枝，去除哪些能够<font color='blue'>提升</font>该规则准确率的<font color='blue'>规则前件</font>
       - 将规则排序成一个序列 (根据规则的<font color='blue'>准确率从高往低排序</font>) 
       - 用<font color='blue'>该序列</font>中的最终规则对样本进行分类（<font color='blue'>依次查看</font>是否满足规则序列）

       注：在规则被剪枝后，它可能<font color='red'>不再能恢复成一棵树</font>

### 2.2 回归算法

#### 2.2.1 线性回归

1. 回归分析(Regression)

   回归分析是描述<font color='blue'>变量间关系</font>的一种统计分析方法

   > 例：在线教育场景
   >
   > - 因变量 Y：在线学习课程满意度 
   >
   > - 自变量 X：平台交互性、教学资源、课程设计

   预测性的建模技术，通常用于<font color='blue'>预测分析</font>，预测的结果多为<font color='blue'>连续值</font>（也可为离散值，二值）

2. 线性回归 (Linear regression)

   因变量和自变量之间是<font color='red'>线性关系</font>，就可以使用线性回归来建模
   $$
   y = b + m_1x_1 + m_2x_2 + \ldots + m_nx_n
   $$
   其中 $m_1, m_2, \ldots, m_n $ 是各个特征的权重，$ x_1, x_2, \ldots, x_n $是对应的特征值。

   线性回归的目的即找到<font color='blue'>最能匹配(解释)数据</font>的<font color='red'>截距</font>和<font color='red'>斜率</font>

3. 如何拟合数据

   假设只有一个因变量和自变量，每个训练样例表示 (𝑥𝑖 , 𝑦𝑖)

   用 $\hat y_i$ 表示根据拟合直线和 x𝑖 对 𝑦𝑖 的<font color='red'>预测</font>值：$\hat y_i=b_1+b_2x_i$

   定义 $e_i=y_i-\hat y_i$ 为<font color='red'>误差项</font>

   ![image-20240131105854324](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240131105854324.png)

   目标：得到一条直线使得对于<font color='blue'>所有训练样例的误差项尽可能小</font>

4. 线性回归的基本假设

   - 自变量与因变量间<font color='blue'>存在线性关系</font>；
   - 数据点之间<font color='blue'>独立</font>；
   - <font color='blue'>自变量之间无共线性</font>，相互独立；
   - <font color='blue'>残差独立</font>,等方差,且符合<font color='blue'>正态分布</font>。

#### 2.2.2 损失函数

1. 损失函数(loss function)

   损失函数（Loss Function）是用于衡量模型预测值与实际值之间差异的函数。

   - **均方误差（Mean Squared Error，MSE）**：用于回归问题，计算预测值与实际值之间的平方差的平均值。
     $$
     \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
     $$

   - **平均绝对误差（Mean Absolute Error，MAE）**：也用于回归问题，计算预测值与实际值之间的绝对差的平均值。
     $$
     \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
     $$

2. 最小二乘法（Least Square, LS)

   为了求解最优的截距和斜率，可以转化为一个针对损失函数的凸优化问题，称为<font color='red'>最小二乘法</font> 
   $$
   \min_{b_1,b_2}:\sum_{i=1}^n(y_i - \hat{y}_i)^2=\sum_{i=1}^n(y_i - b_1-b_2x)^2
   $$
   为求最小值，将上事对$b_1$，$b_2$求偏导，得到：
   $$
   b_2=\frac{\sum_{i=1}^n(x_i-\hat x)(y_i-\hat y)}{\sum_{i=1}^n(x_i-\hat x)^2}\\b_1=\hat y-b_2\hat x
   $$
   $\hat x$ 和 $\hat y$ 分别表示自变量和因变量的均值。

3. 梯度下降法(Gradient Descent, GD)

   沿着损失函数的梯度方向，通过不断更新模型参数来减小损失函数的值，直到达到损失函数的局部最小值或收敛到某个停止条件。

   - **初始化参数：** 随机初始化模型的参数（权重 $b_2$ 和偏置 $b_1$）。

   - **计算梯度：** 计算当前参数下损失函数关于每个参数的梯度（导数）。
     $$
     \frac{\part \sum_{i=1}^ne_i^2}{\part b}
     $$

   - **更新参数：** 沿着梯度的反方向调整参数，通过学习率 $\alpha$ 控制每次更新的步长。
     $$
     b=b-\alpha \frac{\part \sum_{i=1}^ne_i^2}{\part b}
     $$

   - **重复：** 重复步骤2和3，直到满足停止条件，例如达到最大迭代次数或梯度接近零。

   - **输出：** 返回最终的模型参数。

#### 2.2.3 多元线性回归

1. 多元线性回归(Multiple Linear Regression)

   当因变量有多个时，我们可以用矩阵方式表达
   $$
   Y=X\beta +e
   $$
   $Y$ 为输出，$X$ 为输入矩阵，$\beta$ 为回归系数，$e$ 是误差项（残差）。

2. 损失函数

   误差项 $e=y-X\beta$

   损失函数 $\sum_{i=1}^ne_i^2=e^\top e$，$e^\top$表示转置

   求解得到 $\beta =(X^\top X)^{-1}X^\top y$

3. 实例：家庭花销预测

   记录了25个家庭每年在快销品和日常服务上<font color='blue'>总开销($Y$)，每年固定收入($X_2$) ,持有流动资产($X_3$)</font>

   可以构建如下线性回归模型：
   $$
   y_i=\beta_1 +\beta_2x_{i2}+\beta_3x_{i3}+\epsilon_i
   $$
   带入 $\beta =(X^\top X)^{-1}X^\top y$ ，计算得到 $\beta_1=36.789$，$\beta_2=0.332$，$\beta_3=0.125$。

   预测：如果一个家庭每年固定收入为 50K\$、持有流动资产 100K\$，则 预计一年将会花费
   $$
   \hat y_i=36.79=0.332*50+0.125*100=65.96K
   $$

4. 以“误差平方和”为损失函数的优缺点

   - 优点
     - 损失函数是严格的凸函数，有<font color='red'>唯一解</font>
     - 求解过程简单且容易计算
   - 缺点
     - 结果对数据中的"<font color='red'>离群点"(outlier)</font>"非常<font color='red'>敏感</font>
     - 损失函数对于<font color='red'>超过</font>和<font color='red'>低于</font>真实值的预测是等价的

#### 2.2.4 线性回归的相关系数

1. 相关系数r

   定义因变量和自变量之间的<font color='blue'>相关系数 r</font>
   $$
   r=\frac{1}{n-1}\sum_{i=1}^n(\frac{x_i-\hat x}{s_x})(\frac{y_i-\hat y}{s_y})
   $$
   其中 $ \hat x$ 是 $X$ 的均值，$S_x$ 是 $X$ 的标准差$\sqrt{\frac{1}{n-1}\sum (x_i-\hat x)^2}$。

   协方差为$(\frac{x_i-\hat x}{s_x})(\frac{y_i-\hat y}{s_y})$，描述两个变量和Y的<font color='blue'>线性相关程度</font>。

   ![image-20240131114416003](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240131114416003.png)

   相关系数r的绝对值越接近1，线性相关程度越高。

2. 决定系数(coefficient of determination)

   决定系数$R^2$ ,也称作判定系数、拟合优度。

   <font color='blue'>决定系数</font>衡量了<font color='red'>模型对数据的解释程度</font>，y的波动有多少百分比能被x的波动所描述。
   $$
   R^2=1-\frac{\text{MSE}}{\text{VAR}}=\frac{\sum_i (y_i-\hat y)^2}{\sum_i(y_i-\bar y)^2}
   $$
   $y_i$ 为真实值，$\hat y_i$ 为预测值，$\bar y_i$ 为均值。$\text{MSE}$ 是均方误差，$\text{VAR}$是方差。

   $R^2$越接近1，表示回归分析中自变量对因变量的<font color='red'>解释越好</font>

   特别注意：<font color='red'>变量相关 ≠ 存在因果关系</font>。

### 2.3 贝叶斯学习



### 2.4 基于实例的学习



### 2.5 支持向量机（SVM）



## 3 无监督学习



### 3.1 聚类



## 4 集成学习



## 5 深度学习